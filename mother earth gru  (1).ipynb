{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LioOT4yFYzf5"
   },
   "source": [
    "# Next Word Prediction:\n",
    "## Mother Earth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s2QGmzekYzf7"
   },
   "source": [
    "### Importing The Required Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "26XRHKXmYzf8"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, LSTM,GRU,Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZaCy7erhYzf-",
    "outputId": "983cca34-678a-4b33-d41a-d5377a47369e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The First Line:  THE CHILD AND ITS ENEMIES.\n",
      "\n",
      "The Last Line:  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Dataset: http://www.gutenberg.org/cache/epub/5200/pg5200.txt\n",
    "    Remove all the unnecessary data and label it as Metamorphosis-clean.\n",
    "    The starting and ending lines should be as follows.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "file = open(\"C:/Users/Admin/Downloads/Mother_Earth.txt\", \"r\", encoding = \"utf8\")\n",
    "lines = []\n",
    "\n",
    "for i in file:\n",
    "    lines.append(i)\n",
    "    \n",
    "print(\"The First Line: \", lines[0])\n",
    "print(\"The Last Line: \", lines[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIK6FJzgYzgA"
   },
   "source": [
    "### Cleaning the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "ftkvHWw8YzgB",
    "outputId": "d378de2b-4b9d-449c-81f9-41894b4e34a0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'THE CHILD AND ITS ENEMIES.  By EMMA GOLDMAN.   Is the child to be considered as an individuality, or as an object to be moulded according to the whims and fancies of those about it? This seems to me to be the most important question to be answered by parents and educators. And whether the child is to grow from within, whether all that craves expression will '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = \"\"\n",
    "\n",
    "for i in lines:\n",
    "    data = ' '. join(lines)\n",
    "    \n",
    "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '')\n",
    "data[:360]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "rMWQWM_rYzgB",
    "outputId": "162d05a0-73d1-4261-f67a-8ee85c5538af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'THE CHILD AND ITS ENEMIES   By EMMA GOLDMAN    Is the child to be considered as an individuality  or as an object to be moulded according to the whims and fancies of those about it  This seems to me to be the most important question to be answered by parents and educators  And whether the child is to grow from within  whether all that craves expression will be permitted to come forth toward the light of day  or whether it is to be kneaded like dough through external forces  depends upon the prop'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space\n",
    "new_data = data.translate(translator)\n",
    "\n",
    "new_data[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "GqAWz6PWYzgC",
    "outputId": "70e36cf9-eb99-4982-e623-268cdab9797e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'THE CHILD AND ITS ENEMIES. By EMMA GOLDMAN. Is the child to be considered as an individuality, or object moulded according whims and fancies of those about it? This seems me most important question answered by parents educators. And whether is grow from within, all that craves expression will permitted come forth toward light day; it kneaded like dough through external forces, depends upon proper answer this vital question. The longing best noblest our times makes for strongest individualities. '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = []\n",
    "\n",
    "for i in data.split():\n",
    "    if i not in z:\n",
    "        z.append(i)\n",
    "        \n",
    "data = ' '.join(z)\n",
    "data[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_yGZjCL0YzgC"
   },
   "source": [
    "### Tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hE6S8DusYzgC",
    "outputId": "55f7e269-32d0-4865-d1bf-788015e26612"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 30, 7, 222, 449, 88, 450, 89, 31, 5]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "\n",
    "# saving the tokenizer for predict function.\n",
    "pickle.dump(tokenizer, open('tokenizer1.pkl', 'wb'))\n",
    "\n",
    "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
    "sequence_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lKcpEaPKYzgD",
    "outputId": "9835f6a7-0eb7-4166-e017-c11599fc8f00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4234\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dv4ZNCD9YzgE",
    "outputId": "740c50c5-2142-4519-dfce-ac2002dd3315"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Length of sequences are:  6435\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  5,  30],\n",
       "       [ 30,   7],\n",
       "       [  7, 222],\n",
       "       [222, 449],\n",
       "       [449,  88],\n",
       "       [ 88, 450],\n",
       "       [450,  89],\n",
       "       [ 89,  31],\n",
       "       [ 31,   5],\n",
       "       [  5,  30]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = []\n",
    "\n",
    "for i in range(1, len(sequence_data)):\n",
    "    words = sequence_data[i-1:i+1]\n",
    "    sequences.append(words)\n",
    "    \n",
    "print(\"The Length of sequences are: \", len(sequences))\n",
    "sequences = np.array(sequences)\n",
    "sequences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "pw4kd5phYzgF"
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in sequences:\n",
    "    X.append(i[0])\n",
    "    y.append(i[1])\n",
    "    \n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hg5WVVhQYzgF",
    "outputId": "3571ba03-a5dd-4113-aac7-52becb78975f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Data is:  [  5  30   7 222 449]\n",
      "The responses are:  [ 30   7 222 449  88]\n"
     ]
    }
   ],
   "source": [
    "print(\"The Data is: \", X[:5])\n",
    "print(\"The responses are: \", y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dkUuhaEjYzgG",
    "outputId": "3eaf58b9-18b4-41a0-e858-0e82a80be90f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcw1UgR3YzgG"
   },
   "source": [
    "### Creating the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "wHJIxHYIYzgH"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=1))\n",
    "model.add(GRU(1000, return_sequences=True))\n",
    "model.add(GRU(1000))\n",
    "model.add(Dense(1000, activation=\"tanh\"))\n",
    "model.add(Dense(vocab_size, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "mQXcNxx_YzgH",
    "outputId": "8783059b-3e77-44c4-fdad-2434a23127da",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 1, 10)             42340     \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 1, 1000)           3036000   \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 1000)              6006000   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1000)              1001000   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4234)              4238234   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,323,574\n",
      "Trainable params: 14,323,574\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydot in c:\\users\\admin\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pydot) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphviz in c:\\users\\admin\\anaconda3\\lib\\site-packages (0.20.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMpP5WicYzgH"
   },
   "source": [
    "### Plot The Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "fZ7G20NJYzgH",
    "outputId": "558d2b27-7c98-4f2e-964d-8b98780ac1e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "import pydot\n",
    "import graphviz\n",
    "from tensorflow import keras\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "keras.utils.plot_model(model, to_file='model.png', show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fiTtJ7o4YzgI"
   },
   "source": [
    "### Callbacks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "K4oEesXZYzgI"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"nextword1.h5\", monitor='loss', verbose=1,\n",
    "    save_best_only=True, mode='auto')\n",
    "\n",
    "reduce = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose = 1)\n",
    "\n",
    "logdir='logsnextword1'\n",
    "tensorboard_Visualization = TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AB8_eIn8YzgI"
   },
   "source": [
    "### Compile The Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m-u0OU-pYzgI",
    "outputId": "ed06a38b-63d5-4494-e8e2-c82d563c2e87"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.001),metrics=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 8.3473 - accuracy: 4.6620e-04\n",
      "Epoch 1: loss improved from inf to 8.34727, saving model to nextword1.h5\n",
      "101/101 [==============================] - 18s 131ms/step - loss: 8.3473 - accuracy: 4.6620e-04 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 8.3063 - accuracy: 0.0016\n",
      "Epoch 2: loss improved from 8.34727 to 8.30631, saving model to nextword1.h5\n",
      "101/101 [==============================] - 11s 109ms/step - loss: 8.3063 - accuracy: 0.0016 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 8.0503 - accuracy: 0.0019\n",
      "Epoch 3: loss improved from 8.30631 to 8.05032, saving model to nextword1.h5\n",
      "101/101 [==============================] - 10s 102ms/step - loss: 8.0503 - accuracy: 0.0019 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 7.6780 - accuracy: 0.0039\n",
      "Epoch 4: loss improved from 8.05032 to 7.67797, saving model to nextword1.h5\n",
      "101/101 [==============================] - 10s 100ms/step - loss: 7.6780 - accuracy: 0.0039 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 7.2127 - accuracy: 0.0073\n",
      "Epoch 5: loss improved from 7.67797 to 7.21269, saving model to nextword1.h5\n",
      "101/101 [==============================] - 10s 99ms/step - loss: 7.2127 - accuracy: 0.0073 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 6.5277 - accuracy: 0.0169\n",
      "Epoch 6: loss improved from 7.21269 to 6.52765, saving model to nextword1.h5\n",
      "101/101 [==============================] - 10s 96ms/step - loss: 6.5277 - accuracy: 0.0169 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 5.6978 - accuracy: 0.0376\n",
      "Epoch 7: loss improved from 6.52765 to 5.69779, saving model to nextword1.h5\n",
      "101/101 [==============================] - 10s 97ms/step - loss: 5.6978 - accuracy: 0.0376 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 4.8251 - accuracy: 0.0872\n",
      "Epoch 8: loss improved from 5.69779 to 4.82513, saving model to nextword1.h5\n",
      "101/101 [==============================] - 10s 97ms/step - loss: 4.8251 - accuracy: 0.0872 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 3.9926 - accuracy: 0.1644\n",
      "Epoch 9: loss improved from 4.82513 to 3.99257, saving model to nextword1.h5\n",
      "101/101 [==============================] - 10s 97ms/step - loss: 3.9926 - accuracy: 0.1644 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 3.3248 - accuracy: 0.2632\n",
      "Epoch 10: loss improved from 3.99257 to 3.32477, saving model to nextword1.h5\n",
      "101/101 [==============================] - 10s 97ms/step - loss: 3.3248 - accuracy: 0.2632 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 2.7961 - accuracy: 0.3643\n",
      "Epoch 11: loss improved from 3.32477 to 2.79611, saving model to nextword1.h5\n",
      "101/101 [==============================] - 10s 95ms/step - loss: 2.7961 - accuracy: 0.3643 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 2.4488 - accuracy: 0.4490\n",
      "Epoch 12: loss improved from 2.79611 to 2.44880, saving model to nextword1.h5\n",
      "101/101 [==============================] - 10s 98ms/step - loss: 2.4488 - accuracy: 0.4490 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 2.1428 - accuracy: 0.5147\n",
      "Epoch 13: loss improved from 2.44880 to 2.14284, saving model to nextword1.h5\n",
      "101/101 [==============================] - 10s 96ms/step - loss: 2.1428 - accuracy: 0.5147 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.9467 - accuracy: 0.5465\n",
      "Epoch 14: loss improved from 2.14284 to 1.94674, saving model to nextword1.h5\n",
      "101/101 [==============================] - 10s 97ms/step - loss: 1.9467 - accuracy: 0.5465 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.7944 - accuracy: 0.5532\n",
      "Epoch 15: loss improved from 1.94674 to 1.79441, saving model to nextword1.h5\n",
      "101/101 [==============================] - 10s 98ms/step - loss: 1.7944 - accuracy: 0.5532 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.7016 - accuracy: 0.5618\n",
      "Epoch 16: loss improved from 1.79441 to 1.70156, saving model to nextword1.h5\n",
      "101/101 [==============================] - 10s 97ms/step - loss: 1.7016 - accuracy: 0.5618 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.6291 - accuracy: 0.5653\n",
      "Epoch 17: loss improved from 1.70156 to 1.62906, saving model to nextword1.h5\n",
      "101/101 [==============================] - 10s 98ms/step - loss: 1.6291 - accuracy: 0.5653 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.5695 - accuracy: 0.5669\n",
      "Epoch 18: loss improved from 1.62906 to 1.56951, saving model to nextword1.h5\n",
      "101/101 [==============================] - 10s 98ms/step - loss: 1.5695 - accuracy: 0.5669 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.5432 - accuracy: 0.5681\n",
      "Epoch 19: loss improved from 1.56951 to 1.54320, saving model to nextword1.h5\n",
      "101/101 [==============================] - 10s 97ms/step - loss: 1.5432 - accuracy: 0.5681 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.5088 - accuracy: 0.5646\n",
      "Epoch 20: loss improved from 1.54320 to 1.50877, saving model to nextword1.h5\n",
      "101/101 [==============================] - 10s 100ms/step - loss: 1.5088 - accuracy: 0.5646 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.4862 - accuracy: 0.5624\n",
      "Epoch 21: loss improved from 1.50877 to 1.48624, saving model to nextword1.h5\n",
      "101/101 [==============================] - 10s 98ms/step - loss: 1.4862 - accuracy: 0.5624 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.4439 - accuracy: 0.5719\n",
      "Epoch 22: loss improved from 1.48624 to 1.44390, saving model to nextword1.h5\n",
      "101/101 [==============================] - 10s 97ms/step - loss: 1.4439 - accuracy: 0.5719 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.4242 - accuracy: 0.5689\n",
      "Epoch 23: loss improved from 1.44390 to 1.42418, saving model to nextword1.h5\n",
      "101/101 [==============================] - 10s 98ms/step - loss: 1.4242 - accuracy: 0.5689 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.4055 - accuracy: 0.5722\n",
      "Epoch 24: loss improved from 1.42418 to 1.40548, saving model to nextword1.h5\n",
      "101/101 [==============================] - 10s 98ms/step - loss: 1.4055 - accuracy: 0.5722 - lr: 0.0010\n",
      "Epoch 25/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.4057 - accuracy: 0.5660\n",
      "Epoch 25: loss did not improve from 1.40548\n",
      "101/101 [==============================] - 10s 96ms/step - loss: 1.4057 - accuracy: 0.5660 - lr: 0.0010\n",
      "Epoch 26/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.3841 - accuracy: 0.5677\n",
      "Epoch 26: loss improved from 1.40548 to 1.38407, saving model to nextword1.h5\n",
      "101/101 [==============================] - 10s 98ms/step - loss: 1.3841 - accuracy: 0.5677 - lr: 0.0010\n",
      "Epoch 27/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.3477 - accuracy: 0.5726\n",
      "Epoch 27: loss improved from 1.38407 to 1.34769, saving model to nextword1.h5\n",
      "101/101 [==============================] - 10s 99ms/step - loss: 1.3477 - accuracy: 0.5726 - lr: 0.0010\n",
      "Epoch 28/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.3371 - accuracy: 0.5675\n",
      "Epoch 28: loss improved from 1.34769 to 1.33706, saving model to nextword1.h5\n",
      "101/101 [==============================] - 10s 101ms/step - loss: 1.3371 - accuracy: 0.5675 - lr: 0.0010\n",
      "Epoch 29/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.3217 - accuracy: 0.5714\n",
      "Epoch 29: loss improved from 1.33706 to 1.32171, saving model to nextword1.h5\n",
      "101/101 [==============================] - 10s 98ms/step - loss: 1.3217 - accuracy: 0.5714 - lr: 0.0010\n",
      "Epoch 30/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.3120 - accuracy: 0.5730\n",
      "Epoch 30: loss improved from 1.32171 to 1.31203, saving model to nextword1.h5\n",
      "101/101 [==============================] - 10s 98ms/step - loss: 1.3120 - accuracy: 0.5730 - lr: 0.0010\n",
      "Epoch 31/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.3062 - accuracy: 0.5694\n",
      "Epoch 31: loss improved from 1.31203 to 1.30617, saving model to nextword1.h5\n",
      "101/101 [==============================] - 10s 97ms/step - loss: 1.3062 - accuracy: 0.5694 - lr: 0.0010\n",
      "Epoch 32/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.2850 - accuracy: 0.5717\n",
      "Epoch 32: loss improved from 1.30617 to 1.28504, saving model to nextword1.h5\n",
      "101/101 [==============================] - 10s 97ms/step - loss: 1.2850 - accuracy: 0.5717 - lr: 0.0010\n",
      "Epoch 33/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.2860 - accuracy: 0.5669\n",
      "Epoch 33: loss did not improve from 1.28504\n",
      "101/101 [==============================] - 10s 95ms/step - loss: 1.2860 - accuracy: 0.5669 - lr: 0.0010\n",
      "Epoch 34/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.2622 - accuracy: 0.5680\n",
      "Epoch 34: loss improved from 1.28504 to 1.26225, saving model to nextword1.h5\n",
      "101/101 [==============================] - 10s 98ms/step - loss: 1.2622 - accuracy: 0.5680 - lr: 0.0010\n",
      "Epoch 35/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.2680 - accuracy: 0.5685\n",
      "Epoch 35: loss did not improve from 1.26225\n",
      "101/101 [==============================] - 10s 95ms/step - loss: 1.2680 - accuracy: 0.5685 - lr: 0.0010\n",
      "Epoch 36/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.2535 - accuracy: 0.5734\n",
      "Epoch 36: loss improved from 1.26225 to 1.25348, saving model to nextword1.h5\n",
      "101/101 [==============================] - 10s 96ms/step - loss: 1.2535 - accuracy: 0.5734 - lr: 0.0010\n",
      "Epoch 37/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.2491 - accuracy: 0.5697\n",
      "Epoch 37: loss improved from 1.25348 to 1.24910, saving model to nextword1.h5\n",
      "101/101 [==============================] - 9s 94ms/step - loss: 1.2491 - accuracy: 0.5697 - lr: 0.0010\n",
      "Epoch 38/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.2355 - accuracy: 0.5725\n",
      "Epoch 38: loss improved from 1.24910 to 1.23552, saving model to nextword1.h5\n",
      "101/101 [==============================] - 10s 94ms/step - loss: 1.2355 - accuracy: 0.5725 - lr: 0.0010\n",
      "Epoch 39/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.2189 - accuracy: 0.5726\n",
      "Epoch 39: loss improved from 1.23552 to 1.21887, saving model to nextword1.h5\n",
      "101/101 [==============================] - 10s 93ms/step - loss: 1.2189 - accuracy: 0.5726 - lr: 0.0010\n",
      "Epoch 40/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.2162 - accuracy: 0.5739\n",
      "Epoch 40: loss improved from 1.21887 to 1.21624, saving model to nextword1.h5\n",
      "101/101 [==============================] - 10s 93ms/step - loss: 1.2162 - accuracy: 0.5739 - lr: 0.0010\n",
      "Epoch 41/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.2166 - accuracy: 0.5655\n",
      "Epoch 41: loss did not improve from 1.21624\n",
      "101/101 [==============================] - 9s 91ms/step - loss: 1.2166 - accuracy: 0.5655 - lr: 0.0010\n",
      "Epoch 42/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.2032 - accuracy: 0.5739\n",
      "Epoch 42: loss improved from 1.21624 to 1.20319, saving model to nextword1.h5\n",
      "101/101 [==============================] - 10s 96ms/step - loss: 1.2032 - accuracy: 0.5739 - lr: 0.0010\n",
      "Epoch 43/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.2070 - accuracy: 0.5672\n",
      "Epoch 43: loss did not improve from 1.20319\n",
      "101/101 [==============================] - 9s 91ms/step - loss: 1.2070 - accuracy: 0.5672 - lr: 0.0010\n",
      "Epoch 44/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.1860 - accuracy: 0.5725\n",
      "Epoch 44: loss improved from 1.20319 to 1.18598, saving model to nextword1.h5\n",
      "101/101 [==============================] - 9s 93ms/step - loss: 1.1860 - accuracy: 0.5725 - lr: 0.0010\n",
      "Epoch 45/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.1927 - accuracy: 0.5699\n",
      "Epoch 45: loss did not improve from 1.18598\n",
      "101/101 [==============================] - 10s 95ms/step - loss: 1.1927 - accuracy: 0.5699 - lr: 0.0010\n",
      "Epoch 46/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.1796 - accuracy: 0.5726\n",
      "Epoch 46: loss improved from 1.18598 to 1.17955, saving model to nextword1.h5\n",
      "101/101 [==============================] - 9s 93ms/step - loss: 1.1796 - accuracy: 0.5726 - lr: 0.0010\n",
      "Epoch 47/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.1801 - accuracy: 0.5681\n",
      "Epoch 47: loss did not improve from 1.17955\n",
      "101/101 [==============================] - 9s 91ms/step - loss: 1.1801 - accuracy: 0.5681 - lr: 0.0010\n",
      "Epoch 48/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.1669 - accuracy: 0.5680\n",
      "Epoch 48: loss improved from 1.17955 to 1.16694, saving model to nextword1.h5\n",
      "101/101 [==============================] - 10s 95ms/step - loss: 1.1669 - accuracy: 0.5680 - lr: 0.0010\n",
      "Epoch 49/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.1634 - accuracy: 0.5725\n",
      "Epoch 49: loss improved from 1.16694 to 1.16340, saving model to nextword1.h5\n",
      "101/101 [==============================] - 10s 93ms/step - loss: 1.1634 - accuracy: 0.5725 - lr: 0.0010\n",
      "Epoch 50/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.1636 - accuracy: 0.5652\n",
      "Epoch 50: loss did not improve from 1.16340\n",
      "101/101 [==============================] - 9s 91ms/step - loss: 1.1636 - accuracy: 0.5652 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "model=model.fit(X, y, epochs=50, batch_size=64, callbacks=[checkpoint, reduce, tensorboard_Visualization])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(model.history['accuracy'])\n",
    "plt.plot(model.history['loss'])\n",
    "plt.title('model accuracy and loss')\n",
    "plt.ylabel('accuracy and loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['accuracy', 'loss'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202/202 [==============================] - 5s 21ms/step - loss: 0.8668 - accuracy: 0.6628\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8667892813682556, 0.6627816557884216]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"C:/Users/Admin/Downloads/nextword1.h5\")\n",
    "vocab_array = np.array(list(tokenizer.word_index.keys()))\n",
    "\n",
    "# Importing the Libraries\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pickle\n",
    "# Load the model and tokenizer\n",
    "model.load_weights(\"C:/Users/Admin/Downloads/nextword1.h5\")\n",
    "tokenizer = pickle.load(open(\"C:/Users/Admin/Downloads/tokenizer1.pkl\", 'rb'))\n",
    "def make_prediction(text, n_words):\n",
    "    for i in range(n_words):\n",
    "        text_tokenize = tokenizer.texts_to_sequences([text])\n",
    "        text_padded = tf.keras.preprocessing.sequence.pad_sequences(text_tokenize, maxlen=14)\n",
    "        prediction = np.squeeze(np.argmax(model.predict(text_padded), axis=-1))\n",
    "        prediction = str(vocab_array[prediction - 1])\n",
    "        print(vocab_array[np.argsort(model.predict(text_padded)) - 1].ravel()[:-3])\n",
    "        text += \" \" + prediction\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name='embedding_input'), name='embedding_input', description=\"created by layer 'embedding_input'\"), but it was called on an input with incompatible shape (None, 14).\n",
      "1/1 [==============================] - 1s 723ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "['bavaria' 'search' 'shocking' ... 'hopeless' 'bad' 'gates']\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "['bavaria' 'shocking' 'search' ... 'gates' 'hopeless' 'lost']\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "['shocking' 'bavaria' 'polyandry' ... 'can' 'least' 'lost']\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "['shocking' 'bavaria' 'conforming' ... 'well' 'can' 'must']\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "['your' 'conforming' 'shocking' ... 'why' 'existence' 'lost']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'child door must must least least'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_prediction(\"child\",5)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
