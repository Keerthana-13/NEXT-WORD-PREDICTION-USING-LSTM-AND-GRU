{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LioOT4yFYzf5"
   },
   "source": [
    "# Next Word Prediction:\n",
    "## Strange Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s2QGmzekYzf7"
   },
   "source": [
    "### Importing The Required Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "26XRHKXmYzf8"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, LSTM, GRU, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZaCy7erhYzf-",
    "outputId": "983cca34-678a-4b33-d41a-d5377a47369e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The First Line:  STORY OF THE DOOR\n",
      "\n",
      "The Last Line:  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Dataset: http://www.gutenberg.org/cache/epub/5200/pg5200.txt\n",
    "    Remove all the unnecessary data and label it as Metamorphosis-clean.\n",
    "    The starting and ending lines should be as follows.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "file = open(\"C:/Users/Admin/Downloads/The_Strange_Case_of_Dr. Jekyll_and_ Mr. Hy _by_Robert_Louis_Stevenson.txt\", \"r\", encoding = \"utf8\")\n",
    "lines = []\n",
    "\n",
    "for i in file:\n",
    "    lines.append(i)\n",
    "    \n",
    "print(\"The First Line: \", lines[0])\n",
    "print(\"The Last Line: \", lines[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIK6FJzgYzgA"
   },
   "source": [
    "### Cleaning the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "ftkvHWw8YzgB",
    "outputId": "d378de2b-4b9d-449c-81f9-41894b4e34a0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'STORY OF THE DOOR  Mr. Utterson the lawyer was a man of a rugged countenance that was never lighted by a smile; cold, scanty and embarrassed in discourse; backward in sentiment; lean, long, dusty, dreary and yet somehow lovable. At friendly meetings, and when the wine was to his taste, something eminently human beaconed from his eye; something indeed which n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = \"\"\n",
    "\n",
    "for i in lines:\n",
    "    data = ' '. join(lines)\n",
    "    \n",
    "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '')\n",
    "data[:360]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "rMWQWM_rYzgB",
    "outputId": "162d05a0-73d1-4261-f67a-8ee85c5538af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'STORY OF THE DOOR  Mr  Utterson the lawyer was a man of a rugged countenance that was never lighted by a smile  cold  scanty and embarrassed in discourse  backward in sentiment  lean  long  dusty  dreary and yet somehow lovable  At friendly meetings  and when the wine was to his taste  something eminently human beaconed from his eye  something indeed which never found its way into his talk  but which spoke not only in these silent symbols of the after dinner face  but more often and loudly in th'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space\n",
    "new_data = data.translate(translator)\n",
    "\n",
    "new_data[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "GqAWz6PWYzgC",
    "outputId": "70e36cf9-eb99-4982-e623-268cdab9797e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'STORY OF THE DOOR Mr. Utterson the lawyer was a man of rugged countenance that never lighted by smile; cold, scanty and embarrassed in discourse; backward sentiment; lean, long, dusty, dreary yet somehow lovable. At friendly meetings, when wine to his taste, something eminently human beaconed from eye; indeed which found its way into talk, but spoke not only these silent symbols after-dinner face, more often loudly acts life. He austere with himself; drank gin he alone, mortify taste for vintage'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = []\n",
    "\n",
    "for i in data.split():\n",
    "    if i not in z:\n",
    "        z.append(i)\n",
    "        \n",
    "data = ' '.join(z)\n",
    "data[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_yGZjCL0YzgC"
   },
   "source": [
    "### Tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hE6S8DusYzgC",
    "outputId": "55f7e269-32d0-4865-d1bf-788015e26612"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[64, 39, 65, 12, 419, 5, 65, 108, 109, 420]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "\n",
    "# saving the tokenizer for predict function.\n",
    "pickle.dump(tokenizer, open('tokenizer1.pkl', 'wb'))\n",
    "\n",
    "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
    "sequence_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lKcpEaPKYzgD",
    "outputId": "9835f6a7-0eb7-4166-e017-c11599fc8f00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4152\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dv4ZNCD9YzgE",
    "outputId": "740c50c5-2142-4519-dfce-ac2002dd3315"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Length of sequences are:  6439\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 64,  39],\n",
       "       [ 39,  65],\n",
       "       [ 65,  12],\n",
       "       [ 12, 419],\n",
       "       [419,   5],\n",
       "       [  5,  65],\n",
       "       [ 65, 108],\n",
       "       [108, 109],\n",
       "       [109, 420],\n",
       "       [420,  29]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = []\n",
    "\n",
    "for i in range(1, len(sequence_data)):\n",
    "    words = sequence_data[i-1:i+1]\n",
    "    sequences.append(words)\n",
    "    \n",
    "print(\"The Length of sequences are: \", len(sequences))\n",
    "sequences = np.array(sequences)\n",
    "sequences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "pw4kd5phYzgF"
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in sequences:\n",
    "    X.append(i[0])\n",
    "    y.append(i[1])\n",
    "    \n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hg5WVVhQYzgF",
    "outputId": "3571ba03-a5dd-4113-aac7-52becb78975f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Data is:  [ 64  39  65  12 419]\n",
      "The responses are:  [ 39  65  12 419   5]\n"
     ]
    }
   ],
   "source": [
    "print(\"The Data is: \", X[:5])\n",
    "print(\"The responses are: \", y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dkUuhaEjYzgG",
    "outputId": "3eaf58b9-18b4-41a0-e858-0e82a80be90f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcw1UgR3YzgG"
   },
   "source": [
    "### Creating the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "wHJIxHYIYzgH"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=1))\n",
    "model.add(GRU(1000, return_sequences=True))\n",
    "model.add(GRU(1000))\n",
    "model.add(Dense(1000, activation=\"tanh\"))\n",
    "model.add(Dense(vocab_size, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "mQXcNxx_YzgH",
    "outputId": "8783059b-3e77-44c4-fdad-2434a23127da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 1, 10)             41520     \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 1, 1000)           3036000   \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 1000)              6006000   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1000)              1001000   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4152)              4156152   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,240,672\n",
      "Trainable params: 14,240,672\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fiTtJ7o4YzgI"
   },
   "source": [
    "### Callbacks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "K4oEesXZYzgI"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"nextword1.h5\", monitor='loss', verbose=1,\n",
    "    save_best_only=True, mode='auto')\n",
    "\n",
    "reduce = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose = 1)\n",
    "\n",
    "logdir='logsnextword1'\n",
    "tensorboard_Visualization = TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AB8_eIn8YzgI"
   },
   "source": [
    "### Compile The Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m-u0OU-pYzgI",
    "outputId": "ed06a38b-63d5-4494-e8e2-c82d563c2e87",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001),metrics=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhD7eoITYzgJ"
   },
   "source": [
    "### Fit The Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZNKjMMHYYzgJ",
    "outputId": "8cd4ade8-ea4c-4441-cc0f-e8fbf9a60730"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 8.3190 - accuracy: 0.0374\n",
      "Epoch 1: loss improved from inf to 8.31902, saving model to nextword1.h5\n",
      "101/101 [==============================] - 23s 158ms/step - loss: 8.3190 - accuracy: 0.0374 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 8.1627 - accuracy: 0.0436\n",
      "Epoch 2: loss improved from 8.31902 to 8.16274, saving model to nextword1.h5\n",
      "101/101 [==============================] - 12s 119ms/step - loss: 8.1627 - accuracy: 0.0436 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 7.8561 - accuracy: 0.0467\n",
      "Epoch 3: loss improved from 8.16274 to 7.85610, saving model to nextword1.h5\n",
      "101/101 [==============================] - 12s 114ms/step - loss: 7.8561 - accuracy: 0.0467 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 7.5431 - accuracy: 0.0469\n",
      "Epoch 4: loss improved from 7.85610 to 7.54308, saving model to nextword1.h5\n",
      "101/101 [==============================] - 12s 121ms/step - loss: 7.5431 - accuracy: 0.0469 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 7.2412 - accuracy: 0.0472\n",
      "Epoch 5: loss improved from 7.54308 to 7.24119, saving model to nextword1.h5\n",
      "101/101 [==============================] - 12s 120ms/step - loss: 7.2412 - accuracy: 0.0472 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 6.8803 - accuracy: 0.0472\n",
      "Epoch 6: loss improved from 7.24119 to 6.88032, saving model to nextword1.h5\n",
      "101/101 [==============================] - 12s 117ms/step - loss: 6.8803 - accuracy: 0.0472 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 6.4349 - accuracy: 0.0499\n",
      "Epoch 7: loss improved from 6.88032 to 6.43489, saving model to nextword1.h5\n",
      "101/101 [==============================] - 12s 118ms/step - loss: 6.4349 - accuracy: 0.0499 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 5.8971 - accuracy: 0.0592\n",
      "Epoch 8: loss improved from 6.43489 to 5.89708, saving model to nextword1.h5\n",
      "101/101 [==============================] - 11s 111ms/step - loss: 5.8971 - accuracy: 0.0592 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 5.3674 - accuracy: 0.0704\n",
      "Epoch 9: loss improved from 5.89708 to 5.36737, saving model to nextword1.h5\n",
      "101/101 [==============================] - 11s 111ms/step - loss: 5.3674 - accuracy: 0.0704 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 4.8248 - accuracy: 0.1023\n",
      "Epoch 10: loss improved from 5.36737 to 4.82477, saving model to nextword1.h5\n",
      "101/101 [==============================] - 11s 112ms/step - loss: 4.8248 - accuracy: 0.1023 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 4.1905 - accuracy: 0.1511\n",
      "Epoch 11: loss improved from 4.82477 to 4.19047, saving model to nextword1.h5\n",
      "101/101 [==============================] - 11s 110ms/step - loss: 4.1905 - accuracy: 0.1511 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 3.5958 - accuracy: 0.2125\n",
      "Epoch 12: loss improved from 4.19047 to 3.59582, saving model to nextword1.h5\n",
      "101/101 [==============================] - 12s 115ms/step - loss: 3.5958 - accuracy: 0.2125 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 3.0931 - accuracy: 0.2952\n",
      "Epoch 13: loss improved from 3.59582 to 3.09312, saving model to nextword1.h5\n",
      "101/101 [==============================] - 13s 128ms/step - loss: 3.0931 - accuracy: 0.2952 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 2.6462 - accuracy: 0.3982\n",
      "Epoch 14: loss improved from 3.09312 to 2.64617, saving model to nextword1.h5\n",
      "101/101 [==============================] - 12s 115ms/step - loss: 2.6462 - accuracy: 0.3982 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 2.3407 - accuracy: 0.4790\n",
      "Epoch 15: loss improved from 2.64617 to 2.34066, saving model to nextword1.h5\n",
      "101/101 [==============================] - 11s 112ms/step - loss: 2.3407 - accuracy: 0.4790 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 2.0852 - accuracy: 0.5257\n",
      "Epoch 16: loss improved from 2.34066 to 2.08517, saving model to nextword1.h5\n",
      "101/101 [==============================] - 11s 110ms/step - loss: 2.0852 - accuracy: 0.5257 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.9067 - accuracy: 0.5446\n",
      "Epoch 17: loss improved from 2.08517 to 1.90666, saving model to nextword1.h5\n",
      "101/101 [==============================] - 12s 115ms/step - loss: 1.9067 - accuracy: 0.5446 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.8063 - accuracy: 0.5563\n",
      "Epoch 18: loss improved from 1.90666 to 1.80634, saving model to nextword1.h5\n",
      "101/101 [==============================] - 12s 116ms/step - loss: 1.8063 - accuracy: 0.5563 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.7287 - accuracy: 0.5617\n",
      "Epoch 19: loss improved from 1.80634 to 1.72870, saving model to nextword1.h5\n",
      "101/101 [==============================] - 14s 133ms/step - loss: 1.7287 - accuracy: 0.5617 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.6704 - accuracy: 0.5614\n",
      "Epoch 20: loss improved from 1.72870 to 1.67038, saving model to nextword1.h5\n",
      "101/101 [==============================] - 11s 112ms/step - loss: 1.6704 - accuracy: 0.5614 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.6397 - accuracy: 0.5619\n",
      "Epoch 21: loss improved from 1.67038 to 1.63972, saving model to nextword1.h5\n",
      "101/101 [==============================] - 12s 115ms/step - loss: 1.6397 - accuracy: 0.5619 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.6061 - accuracy: 0.5592\n",
      "Epoch 22: loss improved from 1.63972 to 1.60611, saving model to nextword1.h5\n",
      "101/101 [==============================] - 13s 125ms/step - loss: 1.6061 - accuracy: 0.5592 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.5670 - accuracy: 0.5644\n",
      "Epoch 23: loss improved from 1.60611 to 1.56695, saving model to nextword1.h5\n",
      "101/101 [==============================] - 12s 121ms/step - loss: 1.5670 - accuracy: 0.5644 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.5287 - accuracy: 0.5631\n",
      "Epoch 24: loss improved from 1.56695 to 1.52868, saving model to nextword1.h5\n",
      "101/101 [==============================] - 12s 122ms/step - loss: 1.5287 - accuracy: 0.5631 - lr: 0.0010\n",
      "Epoch 25/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.5125 - accuracy: 0.5650\n",
      "Epoch 25: loss improved from 1.52868 to 1.51251, saving model to nextword1.h5\n",
      "101/101 [==============================] - 12s 116ms/step - loss: 1.5125 - accuracy: 0.5650 - lr: 0.0010\n",
      "Epoch 26/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.5124 - accuracy: 0.5627\n",
      "Epoch 26: loss improved from 1.51251 to 1.51239, saving model to nextword1.h5\n",
      "101/101 [==============================] - 12s 120ms/step - loss: 1.5124 - accuracy: 0.5627 - lr: 0.0010\n",
      "Epoch 27/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.4876 - accuracy: 0.5650\n",
      "Epoch 27: loss improved from 1.51239 to 1.48755, saving model to nextword1.h5\n",
      "101/101 [==============================] - 12s 117ms/step - loss: 1.4876 - accuracy: 0.5650 - lr: 0.0010\n",
      "Epoch 28/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.4746 - accuracy: 0.5630\n",
      "Epoch 28: loss improved from 1.48755 to 1.47458, saving model to nextword1.h5\n",
      "101/101 [==============================] - 11s 107ms/step - loss: 1.4746 - accuracy: 0.5630 - lr: 0.0010\n",
      "Epoch 29/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.4513 - accuracy: 0.5630\n",
      "Epoch 29: loss improved from 1.47458 to 1.45129, saving model to nextword1.h5\n",
      "101/101 [==============================] - 11s 110ms/step - loss: 1.4513 - accuracy: 0.5630 - lr: 0.0010\n",
      "Epoch 30/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.4354 - accuracy: 0.5641\n",
      "Epoch 30: loss improved from 1.45129 to 1.43541, saving model to nextword1.h5\n",
      "101/101 [==============================] - 12s 115ms/step - loss: 1.4354 - accuracy: 0.5641 - lr: 0.0010\n",
      "Epoch 31/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.4213 - accuracy: 0.5653\n",
      "Epoch 31: loss improved from 1.43541 to 1.42133, saving model to nextword1.h5\n",
      "101/101 [==============================] - 11s 112ms/step - loss: 1.4213 - accuracy: 0.5653 - lr: 0.0010\n",
      "Epoch 32/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.4198 - accuracy: 0.5664\n",
      "Epoch 32: loss improved from 1.42133 to 1.41980, saving model to nextword1.h5\n",
      "101/101 [==============================] - 11s 111ms/step - loss: 1.4198 - accuracy: 0.5664 - lr: 0.0010\n",
      "Epoch 33/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.4067 - accuracy: 0.5644\n",
      "Epoch 33: loss improved from 1.41980 to 1.40667, saving model to nextword1.h5\n",
      "101/101 [==============================] - 11s 110ms/step - loss: 1.4067 - accuracy: 0.5644 - lr: 0.0010\n",
      "Epoch 34/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.3871 - accuracy: 0.5656\n",
      "Epoch 34: loss improved from 1.40667 to 1.38706, saving model to nextword1.h5\n",
      "101/101 [==============================] - 11s 110ms/step - loss: 1.3871 - accuracy: 0.5656 - lr: 0.0010\n",
      "Epoch 35/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.3830 - accuracy: 0.5678\n",
      "Epoch 35: loss improved from 1.38706 to 1.38295, saving model to nextword1.h5\n",
      "101/101 [==============================] - 11s 106ms/step - loss: 1.3830 - accuracy: 0.5678 - lr: 0.0010\n",
      "Epoch 36/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.3764 - accuracy: 0.5695\n",
      "Epoch 36: loss improved from 1.38295 to 1.37635, saving model to nextword1.h5\n",
      "101/101 [==============================] - 11s 110ms/step - loss: 1.3764 - accuracy: 0.5695 - lr: 0.0010\n",
      "Epoch 37/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.3557 - accuracy: 0.5684\n",
      "Epoch 37: loss improved from 1.37635 to 1.35568, saving model to nextword1.h5\n",
      "101/101 [==============================] - 11s 110ms/step - loss: 1.3557 - accuracy: 0.5684 - lr: 0.0010\n",
      "Epoch 38/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.3579 - accuracy: 0.5675\n",
      "Epoch 38: loss did not improve from 1.35568\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 1.3579 - accuracy: 0.5675 - lr: 0.0010\n",
      "Epoch 39/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.3531 - accuracy: 0.5659\n",
      "Epoch 39: loss improved from 1.35568 to 1.35310, saving model to nextword1.h5\n",
      "101/101 [==============================] - 11s 105ms/step - loss: 1.3531 - accuracy: 0.5659 - lr: 0.0010\n",
      "Epoch 40/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.3541 - accuracy: 0.5641\n",
      "Epoch 40: loss did not improve from 1.35310\n",
      "101/101 [==============================] - 11s 105ms/step - loss: 1.3541 - accuracy: 0.5641 - lr: 0.0010\n",
      "Epoch 41/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.3390 - accuracy: 0.5673\n",
      "Epoch 41: loss improved from 1.35310 to 1.33902, saving model to nextword1.h5\n",
      "101/101 [==============================] - 11s 114ms/step - loss: 1.3390 - accuracy: 0.5673 - lr: 0.0010\n",
      "Epoch 42/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.3347 - accuracy: 0.5647\n",
      "Epoch 42: loss improved from 1.33902 to 1.33474, saving model to nextword1.h5\n",
      "101/101 [==============================] - 11s 112ms/step - loss: 1.3347 - accuracy: 0.5647 - lr: 0.0010\n",
      "Epoch 43/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.3243 - accuracy: 0.5645\n",
      "Epoch 43: loss improved from 1.33474 to 1.32433, saving model to nextword1.h5\n",
      "101/101 [==============================] - 10s 100ms/step - loss: 1.3243 - accuracy: 0.5645 - lr: 0.0010\n",
      "Epoch 44/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.3113 - accuracy: 0.5667\n",
      "Epoch 44: loss improved from 1.32433 to 1.31129, saving model to nextword1.h5\n",
      "101/101 [==============================] - 12s 119ms/step - loss: 1.3113 - accuracy: 0.5667 - lr: 0.0010\n",
      "Epoch 45/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.3103 - accuracy: 0.5658\n",
      "Epoch 45: loss improved from 1.31129 to 1.31033, saving model to nextword1.h5\n",
      "101/101 [==============================] - 10s 99ms/step - loss: 1.3103 - accuracy: 0.5658 - lr: 0.0010\n",
      "Epoch 46/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.2996 - accuracy: 0.5662\n",
      "Epoch 46: loss improved from 1.31033 to 1.29958, saving model to nextword1.h5\n",
      "101/101 [==============================] - 10s 102ms/step - loss: 1.2996 - accuracy: 0.5662 - lr: 0.0010\n",
      "Epoch 47/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.3003 - accuracy: 0.5662\n",
      "Epoch 47: loss did not improve from 1.29958\n",
      "101/101 [==============================] - 10s 97ms/step - loss: 1.3003 - accuracy: 0.5662 - lr: 0.0010\n",
      "Epoch 48/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.2938 - accuracy: 0.5695\n",
      "Epoch 48: loss improved from 1.29958 to 1.29379, saving model to nextword1.h5\n",
      "101/101 [==============================] - 10s 102ms/step - loss: 1.2938 - accuracy: 0.5695 - lr: 0.0010\n",
      "Epoch 49/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.2933 - accuracy: 0.5672\n",
      "Epoch 49: loss improved from 1.29379 to 1.29333, saving model to nextword1.h5\n",
      "101/101 [==============================] - 10s 100ms/step - loss: 1.2933 - accuracy: 0.5672 - lr: 0.0010\n",
      "Epoch 50/50\n",
      "101/101 [==============================] - ETA: 0s - loss: 1.2884 - accuracy: 0.5653\n",
      "Epoch 50: loss improved from 1.29333 to 1.28835, saving model to nextword1.h5\n",
      "101/101 [==============================] - 10s 103ms/step - loss: 1.2884 - accuracy: 0.5653 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "model=model.fit(X, y, epochs=50, batch_size=64, callbacks=[checkpoint, reduce, tensorboard_Visualization])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(model.history['accuracy'])\n",
    "plt.plot(model.history['loss'])\n",
    "plt.title('model accuracy and loss')\n",
    "plt.ylabel('accuracy and loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['accuracy', 'loss'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202/202 [==============================] - 5s 21ms/step - loss: 1.0062 - accuracy: 0.6544\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0062423944473267, 0.654449462890625]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"C:/Users/Admin/Downloads/nextword2.h5\")\n",
    "vocab_array = np.array(list(tokenizer.word_index.keys()))\n",
    "\n",
    "# Importing the Libraries\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pickle\n",
    "# Load the model and tokenizer\n",
    "model.load_weights(\"C:/Users/Admin/Downloads/nextword1.h5\")\n",
    "tokenizer = pickle.load(open(\"C:/Users/Admin/Downloads/tokenizer1.pkl\", 'rb'))\n",
    "def make_prediction(text, n_words):\n",
    "    for i in range(n_words):\n",
    "        text_tokenize = tokenizer.texts_to_sequences([text])\n",
    "        text_padded = tf.keras.preprocessing.sequence.pad_sequences(text_tokenize, maxlen=14)\n",
    "        prediction = np.squeeze(np.argmax(model.predict(text_padded), axis=-1))\n",
    "        prediction = str(vocab_array[prediction - 1])\n",
    "        print(vocab_array[np.argsort(model.predict(text_padded)) - 1].ravel()[:-3])\n",
    "        text += \" \" + prediction\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name='embedding_input'), name='embedding_input', description=\"created by layer 'embedding_input'\"), but it was called on an input with incompatible shape (None, 14).\n",
      "1/1 [==============================] - 1s 847ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "['shock' 'delirium' 'me—something' ... 'pedant' 'fit' 'pleasant']\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "['neat' 'shock' 'me—something' ... 'pleasant' 'attendants' 'fit']\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "['shock' 'neat' 'me—something' ... 'been' 'fit' 'pleasant']\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "['against' 'shock' 'neat' ... 'volume' 'been' 'pleasant']\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "['against' 'shock' 'oration' ... 'looked' 'volume' 'forest']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'door moment door door door door'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_prediction(\"door\",5)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
