{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LioOT4yFYzf5"
   },
   "source": [
    "# Next Word Prediction:\n",
    "## Metamorphopsis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s2QGmzekYzf7"
   },
   "source": [
    "### Importing The Required Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "26XRHKXmYzf8"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, LSTM,GRU,Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZaCy7erhYzf-",
    "outputId": "983cca34-678a-4b33-d41a-d5377a47369e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The First Line:  One morning, when Gregor Samsa woke from troubled dreams, he found\n",
      "\n",
      "The Last Line:  first to get up and stretch out her young body.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Dataset: http://www.gutenberg.org/cache/epub/5200/pg5200.txt\n",
    "    Remove all the unnecessary data and label it as Metamorphosis-clean.\n",
    "    The starting and ending lines should be as follows.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "file = open(\"C:/Users/Admin/Downloads/metamorphosis_clean.txt\", \"r\", encoding = \"utf8\")\n",
    "lines = []\n",
    "\n",
    "for i in file:\n",
    "    lines.append(i)\n",
    "    \n",
    "print(\"The First Line: \", lines[0])\n",
    "print(\"The Last Line: \", lines[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIK6FJzgYzgA"
   },
   "source": [
    "### Cleaning the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "ftkvHWw8YzgB",
    "outputId": "d378de2b-4b9d-449c-81f9-41894b4e34a0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin.  He lay on his armour-like back, and if he lifted his head a little he could see his brown belly, slightly domed and divided by arches into stiff sections.  The bedding was hardly able to cover it and seemed ready to slide off any moment.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = \"\"\n",
    "\n",
    "for i in lines:\n",
    "    data = ' '. join(lines)\n",
    "    \n",
    "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '')\n",
    "data[:360]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "rMWQWM_rYzgB",
    "outputId": "162d05a0-73d1-4261-f67a-8ee85c5538af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One morning  when Gregor Samsa woke from troubled dreams  he found himself transformed in his bed into a horrible vermin   He lay on his armour like back  and if he lifted his head a little he could see his brown belly  slightly domed and divided by arches into stiff sections   The bedding was hardly able to cover it and seemed ready to slide off any moment   His many legs  pitifully thin compared with the size of the rest of him  waved about helplessly as he looked    What s happened to me   he'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space\n",
    "new_data = data.translate(translator)\n",
    "\n",
    "new_data[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "GqAWz6PWYzgC",
    "outputId": "70e36cf9-eb99-4982-e623-268cdab9797e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin. He lay on armour-like back, and if lifted head little could see brown belly, slightly domed divided by arches stiff sections. The bedding was hardly able to cover it seemed ready slide off any moment. His many legs, pitifully thin compared with the size of rest him, waved about helplessly as looked. \"What\\'s happened me?\" thought. It wasn\\'t dream. room, proper human room altho'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = []\n",
    "\n",
    "for i in data.split():\n",
    "    if i not in z:\n",
    "        z.append(i)\n",
    "        \n",
    "data = ' '.join(z)\n",
    "data[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_yGZjCL0YzgC"
   },
   "source": [
    "### Tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hE6S8DusYzgC",
    "outputId": "55f7e269-32d0-4865-d1bf-788015e26612"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17, 53, 293, 2, 18, 729, 135, 730, 294, 8]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "\n",
    "# saving the tokenizer for predict function.\n",
    "pickle.dump(tokenizer, open('tokenizer2.pkl', 'wb'))\n",
    "\n",
    "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
    "sequence_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lKcpEaPKYzgD",
    "outputId": "9835f6a7-0eb7-4166-e017-c11599fc8f00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2617\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dv4ZNCD9YzgE",
    "outputId": "740c50c5-2142-4519-dfce-ac2002dd3315"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Length of sequences are:  3889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 17,  53],\n",
       "       [ 53, 293],\n",
       "       [293,   2],\n",
       "       [  2,  18],\n",
       "       [ 18, 729],\n",
       "       [729, 135],\n",
       "       [135, 730],\n",
       "       [730, 294],\n",
       "       [294,   8],\n",
       "       [  8, 731]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = []\n",
    "\n",
    "for i in range(1, len(sequence_data)):\n",
    "    words = sequence_data[i-1:i+1]\n",
    "    sequences.append(words)\n",
    "    \n",
    "print(\"The Length of sequences are: \", len(sequences))\n",
    "sequences = np.array(sequences)\n",
    "sequences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "pw4kd5phYzgF"
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in sequences:\n",
    "    X.append(i[0])\n",
    "    y.append(i[1])\n",
    "    \n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hg5WVVhQYzgF",
    "outputId": "3571ba03-a5dd-4113-aac7-52becb78975f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Data is:  [ 17  53 293   2  18]\n",
      "The responses are:  [ 53 293   2  18 729]\n"
     ]
    }
   ],
   "source": [
    "print(\"The Data is: \", X[:5])\n",
    "print(\"The responses are: \", y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dkUuhaEjYzgG",
    "outputId": "3eaf58b9-18b4-41a0-e858-0e82a80be90f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcw1UgR3YzgG"
   },
   "source": [
    "### Creating the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "wHJIxHYIYzgH"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=1))\n",
    "model.add(GRU(1000, return_sequences=True))\n",
    "model.add(GRU(1000))\n",
    "model.add(Dense(1000, activation=\"tanh\"))\n",
    "model.add(Dense(vocab_size, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "mQXcNxx_YzgH",
    "outputId": "8783059b-3e77-44c4-fdad-2434a23127da",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 1, 10)             26170     \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 1, 1000)           3036000   \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 1000)              6006000   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1000)              1001000   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2617)              2619617   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,688,787\n",
      "Trainable params: 12,688,787\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydot in c:\\users\\admin\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pydot) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphviz in c:\\users\\admin\\anaconda3\\lib\\site-packages (0.20.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMpP5WicYzgH"
   },
   "source": [
    "### Plot The Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "fZ7G20NJYzgH",
    "outputId": "558d2b27-7c98-4f2e-964d-8b98780ac1e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "import pydot\n",
    "import graphviz\n",
    "from tensorflow import keras\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "keras.utils.plot_model(model, to_file='model.png', show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fiTtJ7o4YzgI"
   },
   "source": [
    "### Callbacks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "K4oEesXZYzgI"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"nextword2.h5\", monitor='loss', verbose=1,\n",
    "    save_best_only=True, mode='auto')\n",
    "\n",
    "reduce = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose = 1)\n",
    "\n",
    "logdir='logsnextword1'\n",
    "tensorboard_Visualization = TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AB8_eIn8YzgI"
   },
   "source": [
    "### Compile The Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m-u0OU-pYzgI",
    "outputId": "ed06a38b-63d5-4494-e8e2-c82d563c2e87"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.001),metrics=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 7.8690 - accuracy: 0.0000e+00\n",
      "Epoch 1: loss improved from inf to 7.86903, saving model to nextword2.h5\n",
      "61/61 [==============================] - 13s 129ms/step - loss: 7.8690 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 7.8614 - accuracy: 0.0013\n",
      "Epoch 2: loss improved from 7.86903 to 7.86136, saving model to nextword2.h5\n",
      "61/61 [==============================] - 5s 87ms/step - loss: 7.8614 - accuracy: 0.0013 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 7.7687 - accuracy: 0.0013\n",
      "Epoch 3: loss improved from 7.86136 to 7.76873, saving model to nextword2.h5\n",
      "61/61 [==============================] - 6s 90ms/step - loss: 7.7687 - accuracy: 0.0013 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 7.4753 - accuracy: 0.0036\n",
      "Epoch 4: loss improved from 7.76873 to 7.47531, saving model to nextword2.h5\n",
      "61/61 [==============================] - 5s 88ms/step - loss: 7.4753 - accuracy: 0.0036 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 7.0093 - accuracy: 0.0090\n",
      "Epoch 5: loss improved from 7.47531 to 7.00927, saving model to nextword2.h5\n",
      "61/61 [==============================] - 5s 87ms/step - loss: 7.0093 - accuracy: 0.0090 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 6.5151 - accuracy: 0.0131\n",
      "Epoch 6: loss improved from 7.00927 to 6.51513, saving model to nextword2.h5\n",
      "61/61 [==============================] - 5s 88ms/step - loss: 6.5151 - accuracy: 0.0131 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 5.9221 - accuracy: 0.0206\n",
      "Epoch 7: loss improved from 6.51513 to 5.92210, saving model to nextword2.h5\n",
      "61/61 [==============================] - 6s 92ms/step - loss: 5.9221 - accuracy: 0.0206 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 5.1712 - accuracy: 0.0543\n",
      "Epoch 8: loss improved from 5.92210 to 5.17116, saving model to nextword2.h5\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 5.1712 - accuracy: 0.0543 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 4.2628 - accuracy: 0.1329\n",
      "Epoch 9: loss improved from 5.17116 to 4.26285, saving model to nextword2.h5\n",
      "61/61 [==============================] - 6s 103ms/step - loss: 4.2628 - accuracy: 0.1329 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 3.4364 - accuracy: 0.2319\n",
      "Epoch 10: loss improved from 4.26285 to 3.43639, saving model to nextword2.h5\n",
      "61/61 [==============================] - 5s 87ms/step - loss: 3.4364 - accuracy: 0.2319 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.7873 - accuracy: 0.3590\n",
      "Epoch 11: loss improved from 3.43639 to 2.78732, saving model to nextword2.h5\n",
      "61/61 [==============================] - 5s 86ms/step - loss: 2.7873 - accuracy: 0.3590 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.3297 - accuracy: 0.4569\n",
      "Epoch 12: loss improved from 2.78732 to 2.32969, saving model to nextword2.h5\n",
      "61/61 [==============================] - 5s 86ms/step - loss: 2.3297 - accuracy: 0.4569 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 2.0113 - accuracy: 0.5351\n",
      "Epoch 13: loss improved from 2.32969 to 2.01133, saving model to nextword2.h5\n",
      "61/61 [==============================] - 5s 86ms/step - loss: 2.0113 - accuracy: 0.5351 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.7998 - accuracy: 0.5554\n",
      "Epoch 14: loss improved from 2.01133 to 1.79984, saving model to nextword2.h5\n",
      "61/61 [==============================] - 6s 91ms/step - loss: 1.7998 - accuracy: 0.5554 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.6388 - accuracy: 0.5683\n",
      "Epoch 15: loss improved from 1.79984 to 1.63875, saving model to nextword2.h5\n",
      "61/61 [==============================] - 6s 91ms/step - loss: 1.6388 - accuracy: 0.5683 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.5486 - accuracy: 0.5675\n",
      "Epoch 16: loss improved from 1.63875 to 1.54865, saving model to nextword2.h5\n",
      "61/61 [==============================] - 6s 89ms/step - loss: 1.5486 - accuracy: 0.5675 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.4887 - accuracy: 0.5683\n",
      "Epoch 17: loss improved from 1.54865 to 1.48872, saving model to nextword2.h5\n",
      "61/61 [==============================] - 6s 90ms/step - loss: 1.4887 - accuracy: 0.5683 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.4422 - accuracy: 0.5744\n",
      "Epoch 18: loss improved from 1.48872 to 1.44223, saving model to nextword2.h5\n",
      "61/61 [==============================] - 5s 89ms/step - loss: 1.4422 - accuracy: 0.5744 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.3902 - accuracy: 0.5703\n",
      "Epoch 19: loss improved from 1.44223 to 1.39024, saving model to nextword2.h5\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 1.3902 - accuracy: 0.5703 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.3396 - accuracy: 0.5780\n",
      "Epoch 20: loss improved from 1.39024 to 1.33963, saving model to nextword2.h5\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 1.3396 - accuracy: 0.5780 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.3327 - accuracy: 0.5721\n",
      "Epoch 21: loss improved from 1.33963 to 1.33272, saving model to nextword2.h5\n",
      "61/61 [==============================] - 6s 92ms/step - loss: 1.3327 - accuracy: 0.5721 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.2885 - accuracy: 0.5819\n",
      "Epoch 22: loss improved from 1.33272 to 1.28854, saving model to nextword2.h5\n",
      "61/61 [==============================] - 5s 88ms/step - loss: 1.2885 - accuracy: 0.5819 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.3038 - accuracy: 0.5739\n",
      "Epoch 23: loss did not improve from 1.28854\n",
      "61/61 [==============================] - 5s 86ms/step - loss: 1.3038 - accuracy: 0.5739 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.2767 - accuracy: 0.5726\n",
      "Epoch 24: loss improved from 1.28854 to 1.27666, saving model to nextword2.h5\n",
      "61/61 [==============================] - 5s 86ms/step - loss: 1.2767 - accuracy: 0.5726 - lr: 0.0010\n",
      "Epoch 25/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.2422 - accuracy: 0.5729\n",
      "Epoch 25: loss improved from 1.27666 to 1.24218, saving model to nextword2.h5\n",
      "61/61 [==============================] - 6s 89ms/step - loss: 1.2422 - accuracy: 0.5729 - lr: 0.0010\n",
      "Epoch 26/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.2263 - accuracy: 0.5711\n",
      "Epoch 26: loss improved from 1.24218 to 1.22626, saving model to nextword2.h5\n",
      "61/61 [==============================] - 5s 88ms/step - loss: 1.2263 - accuracy: 0.5711 - lr: 0.0010\n",
      "Epoch 27/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.2073 - accuracy: 0.5775\n",
      "Epoch 27: loss improved from 1.22626 to 1.20730, saving model to nextword2.h5\n",
      "61/61 [==============================] - 5s 88ms/step - loss: 1.2073 - accuracy: 0.5775 - lr: 0.0010\n",
      "Epoch 28/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.1957 - accuracy: 0.5786\n",
      "Epoch 28: loss improved from 1.20730 to 1.19569, saving model to nextword2.h5\n",
      "61/61 [==============================] - 5s 88ms/step - loss: 1.1957 - accuracy: 0.5786 - lr: 0.0010\n",
      "Epoch 29/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.1786 - accuracy: 0.5760\n",
      "Epoch 29: loss improved from 1.19569 to 1.17857, saving model to nextword2.h5\n",
      "61/61 [==============================] - 6s 93ms/step - loss: 1.1786 - accuracy: 0.5760 - lr: 0.0010\n",
      "Epoch 30/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.1773 - accuracy: 0.5814\n",
      "Epoch 30: loss improved from 1.17857 to 1.17734, saving model to nextword2.h5\n",
      "61/61 [==============================] - 5s 90ms/step - loss: 1.1773 - accuracy: 0.5814 - lr: 0.0010\n",
      "Epoch 31/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.1493 - accuracy: 0.5806\n",
      "Epoch 31: loss improved from 1.17734 to 1.14934, saving model to nextword2.h5\n",
      "61/61 [==============================] - 5s 86ms/step - loss: 1.1493 - accuracy: 0.5806 - lr: 0.0010\n",
      "Epoch 32/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.1539 - accuracy: 0.5786\n",
      "Epoch 32: loss did not improve from 1.14934\n",
      "61/61 [==============================] - 5s 85ms/step - loss: 1.1539 - accuracy: 0.5786 - lr: 0.0010\n",
      "Epoch 33/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.1360 - accuracy: 0.5752\n",
      "Epoch 33: loss improved from 1.14934 to 1.13602, saving model to nextword2.h5\n",
      "61/61 [==============================] - 5s 88ms/step - loss: 1.1360 - accuracy: 0.5752 - lr: 0.0010\n",
      "Epoch 34/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.1462 - accuracy: 0.5752\n",
      "Epoch 34: loss did not improve from 1.13602\n",
      "61/61 [==============================] - 5s 85ms/step - loss: 1.1462 - accuracy: 0.5752 - lr: 0.0010\n",
      "Epoch 35/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.1101 - accuracy: 0.5798\n",
      "Epoch 35: loss improved from 1.13602 to 1.11011, saving model to nextword2.h5\n",
      "61/61 [==============================] - 5s 86ms/step - loss: 1.1101 - accuracy: 0.5798 - lr: 0.0010\n",
      "Epoch 36/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.1124 - accuracy: 0.5832\n",
      "Epoch 36: loss did not improve from 1.11011\n",
      "61/61 [==============================] - 5s 85ms/step - loss: 1.1124 - accuracy: 0.5832 - lr: 0.0010\n",
      "Epoch 37/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.0958 - accuracy: 0.5773\n",
      "Epoch 37: loss improved from 1.11011 to 1.09584, saving model to nextword2.h5\n",
      "61/61 [==============================] - 5s 86ms/step - loss: 1.0958 - accuracy: 0.5773 - lr: 0.0010\n",
      "Epoch 38/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.0949 - accuracy: 0.5847\n",
      "Epoch 38: loss improved from 1.09584 to 1.09490, saving model to nextword2.h5\n",
      "61/61 [==============================] - 6s 91ms/step - loss: 1.0949 - accuracy: 0.5847 - lr: 0.0010\n",
      "Epoch 39/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.1011 - accuracy: 0.5832\n",
      "Epoch 39: loss did not improve from 1.09490\n",
      "61/61 [==============================] - 5s 83ms/step - loss: 1.1011 - accuracy: 0.5832 - lr: 0.0010\n",
      "Epoch 40/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.0921 - accuracy: 0.5793\n",
      "Epoch 40: loss improved from 1.09490 to 1.09207, saving model to nextword2.h5\n",
      "61/61 [==============================] - 5s 86ms/step - loss: 1.0921 - accuracy: 0.5793 - lr: 0.0010\n",
      "Epoch 41/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.0910 - accuracy: 0.5721\n",
      "Epoch 41: loss improved from 1.09207 to 1.09100, saving model to nextword2.h5\n",
      "61/61 [==============================] - 6s 90ms/step - loss: 1.0910 - accuracy: 0.5721 - lr: 0.0010\n",
      "Epoch 42/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.0799 - accuracy: 0.5760\n",
      "Epoch 42: loss improved from 1.09100 to 1.07995, saving model to nextword2.h5\n",
      "61/61 [==============================] - 5s 86ms/step - loss: 1.0799 - accuracy: 0.5760 - lr: 0.0010\n",
      "Epoch 43/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.0766 - accuracy: 0.5780\n",
      "Epoch 43: loss improved from 1.07995 to 1.07660, saving model to nextword2.h5\n",
      "61/61 [==============================] - 5s 86ms/step - loss: 1.0766 - accuracy: 0.5780 - lr: 0.0010\n",
      "Epoch 44/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.0801 - accuracy: 0.5768\n",
      "Epoch 44: loss did not improve from 1.07660\n",
      "61/61 [==============================] - 5s 83ms/step - loss: 1.0801 - accuracy: 0.5768 - lr: 0.0010\n",
      "Epoch 45/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.0619 - accuracy: 0.5780\n",
      "Epoch 45: loss improved from 1.07660 to 1.06186, saving model to nextword2.h5\n",
      "61/61 [==============================] - 5s 86ms/step - loss: 1.0619 - accuracy: 0.5780 - lr: 0.0010\n",
      "Epoch 46/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.0536 - accuracy: 0.5778\n",
      "Epoch 46: loss improved from 1.06186 to 1.05365, saving model to nextword2.h5\n",
      "61/61 [==============================] - 5s 88ms/step - loss: 1.0536 - accuracy: 0.5778 - lr: 0.0010\n",
      "Epoch 47/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.0582 - accuracy: 0.5757\n",
      "Epoch 47: loss did not improve from 1.05365\n",
      "61/61 [==============================] - 5s 85ms/step - loss: 1.0582 - accuracy: 0.5757 - lr: 0.0010\n",
      "Epoch 48/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.0510 - accuracy: 0.5804\n",
      "Epoch 48: loss improved from 1.05365 to 1.05096, saving model to nextword2.h5\n",
      "61/61 [==============================] - 6s 90ms/step - loss: 1.0510 - accuracy: 0.5804 - lr: 0.0010\n",
      "Epoch 49/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.0374 - accuracy: 0.5775\n",
      "Epoch 49: loss improved from 1.05096 to 1.03741, saving model to nextword2.h5\n",
      "61/61 [==============================] - 5s 86ms/step - loss: 1.0374 - accuracy: 0.5775 - lr: 0.0010\n",
      "Epoch 50/50\n",
      "61/61 [==============================] - ETA: 0s - loss: 1.0360 - accuracy: 0.5811\n",
      "Epoch 50: loss improved from 1.03741 to 1.03605, saving model to nextword2.h5\n",
      "61/61 [==============================] - 5s 83ms/step - loss: 1.0360 - accuracy: 0.5811 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "model=model.fit(X, y, epochs=50, batch_size=64, callbacks=[checkpoint, reduce, tensorboard_Visualization])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEWCAYAAABsY4yMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAwcklEQVR4nO3deZwcdZ3/8dene87cB5OEXIQEAoSQAxJA0EBCBBSQQ1kPQMkiWfypKx4ouLrisSu7rseuurpRuUQQlsMDlSMcBlYUcnEk4QwJGZKQ+07m6P78/vhWz/QMk0zPZGpqpuf9fDzqUdVd1VWf6p751Le+VfX9mrsjIiLFJ5V0ACIiEg8leBGRIqUELyJSpJTgRUSKlBK8iEiRUoIXESlSSvASGzO72cy+VeCyq8xsdtwx9RRmNsbM3MxK9jNf33cPoAQvIlKklOBFWrG/UrBIV6cE38NFp+rXmNlzZrbbzH5hZkPN7E9mttPM5pvZwLzl32dmy8xsm5k9bmbH5M2bamaLo8/dCVQ029a5ZrY0+uxfzGxSgTGeY2ZLzGyHma0xs+ubzX9ntL5t0fzLo/crzey7ZrbazLab2ZPRe6ebWXUL38PsaPp6M7vbzG4zsx3A5WZ2opk9FW1jnZn9yMzK8j5/rJk9bGZbzOwtM/uymQ0zsz1mNjhvuRPMbKOZlbawn61tw83sKjN7xcy2mtmPzcyieWkz+w8z22RmK4FzCvluo8+Wm9kPzGxtNPzAzMqjeYeY2f1RTFvM7AkzS0XzvmRmb0a/90tmdkah25RO4u4aevAArAL+CgwFRgAbgMXAVKAceBT4WrTseGA38G6gFPgi8CpQFg2rgc9G8z4A1AHfij57fLTuk4A08LFo2+V5cczeT4ynA8cRCiSTgLeAC6J5o4GdwIej7Q4GpkTzfgw8Hu1XGjgl2qfTgeoWvofZ0fT1UewXRNusBE4ATgZKgDHACuDqaPm+wDrg84SDWl/gpGjeH4FP5G3n+8AP97Of+91GNN+B+4EB0X5vBM6O5l0FvAiMAgYBj0XLlxzgd8/t7zeiv4EhQBXwF+Cb0bxvAz+NvttS4F2AAUcBa4Dh0XJjgHFJ/z1raPY7Jx2AhoT/AMI/+iV5r+8BfpL3+tPAb6LprwJ35c1LAW9GCXMGsBawvPl/oTHB/ySXNPLmvwSclhdHiwm+hZh/AHw/mr4OuK+FZVLAXmByC/NOp/UEv6CVGK7ObZdwcFmyn+U+CPxfNJ0G1gMnFrifDduIXjvwzrzXdwHXRtOPAlflzTuzDQn+NeC9efPOAlZF098Afgsc0ezzRxAO2LOB0qT/jjW0PKiKRiCUiHP2tvC6TzQ9nFBKB8Dds4RS3Iho3pse/fdHVudNHwZ8PjrV32Zm2wilzeGtBWdmJ5nZY1HVxnZCafWQaPYoQoJq7hBCabqleYVY0yyG8VFVxfqo2uZfC4gBQnKcYGZjCWc+29396ZYWbGUbOevzpvfQ9LfJjzn/u29Nk981ms79Lt8hnKU9ZGYrzexaAHd/lXAAuh7YYGa/NrNWf0vpXErw0hZrCYkagKj+dxShFL8OGJGrE46MzpteA/yLuw/IG3q5+x0FbPd24HfAKHfvT6gyyG1nDTCuhc9sAvbtZ95uoFfefqQJVRP5mjez+hNCFciR7t4P+HIBMeDu+wgl7UuAy4BftrRcAdtozTrCb5Ezen8LtqDJ7xp9di2Au+9098+7+1jgPOBzubp2d7/d3d8ZfdaBf2vDNqUTKMFLW9wFnGNmZ0QXCT8P1BCqYp4C6oF/NLMSM7sIODHvsz8DropK42ZmvaOLp30L2G5fYIu77zOzE4GP5M37FTDbzP4u2u5gM5sSnV3cCHzPzIZHFyHfEV08fBmoiLZfCnyFUDffWgw7gF1mdjTwibx59wPDzOzq6IJlXzM7KW/+rcDlwPuA29q5jdbcRfjuR1q4KH5tGz57B/AVM6sys0OAf87FaeHC+BHRgXsHkAEyZnaUmc2Kvs99hDO9TBu2KZ1ACV4K5u4vAZcCPySUkM8DznP3WnevBS4iJLKthLrne/M+uxC4EvhRNP/VaNlC/D/gG2a2k5B87spb7xvAewkHmy3AUmByNPsLwPPAM9G8fwNS7r49WufPCWcfu4Emd9W04AuEA8tOwsHqzrwYdhKqX84jVKG8AszMm/9/QBZY7O6r2rONAvwMeBB4lnCR/N4DL97Et4CFwHOE72tx9B7AkcB8YBfhIP7f7v444YB4A+HvYD3hAu2X27BN6QTWtMpUROJgZo8Ct7v7z5OORXoOJXiRmJnZdOBhwjWEnUnHIz2HqmhEYmRmtxCqOK5WcpfOphK8iEiRUgleRKRIdalGlA455BAfM2ZM0mGIiHQbixYt2uTuzZ/jALpYgh8zZgwLFy5MOgwRkW7DzPb71LKqaEREipQSvIhIkYo1wZvZZy20Hf6Cmd1hZhWtf0pERDpCbHXwZjYC+EdggrvvNbO7gA8BN7dlPXV1dVRXV7Nv374Yoix+FRUVjBw5ktLSt/UvISJFLu6LrCVApZnVEVrvW9vWFVRXV9O3b1/GjBlD04YKpTXuzubNm6murubwww9POhwR6WSxVdG4+5vAfwBvEJoy3e7uDzVfzszmmtlCM1u4cePGt61n3759DB48WMm9HcyMwYMH6+xHpIeKLcFHTZaeDxxO6Dygt5ld2nw5d5/n7tPcfVpVVYu3ciq5HwR9dyI9V5xVNLOB1919I4CZ3UvoE/NA7WG3z86okxszIBXGFo1LKqC0ssM3KSLS1cWZ4N8ATjazXoTOAM4gtDnd8Xa9BZ7d//ySSug1CCoHQbpLPdslIhKb2LKdu//NzO4mdB5QDywB5sWysUMng3tI8rkx0XTNLti7GXa8CTvWQkU/qBwMFX1DKb+LqK+vp6REBx8R6TixZjh3/5q7H+3uE939MneviW1jZpBKhxJ6SVlj1UyfKqg6Ogy9q6B2N2xdCRtWQKauoFVfcMEFnHDCCRx77LHMmxeOUQ888ADHH388kydP5owzzgBg165dzJkzh+OOO45JkyZxzz33ANCnT5+Gdd19991cfvnlAFx++eV87nOfY+bMmXzpS1/i6aef5pRTTmHq1KmccsopvPTSSwBkMhm+8IUvNKz3hz/8IY888ggXXnhhw3offvhhLrroooP+GkWkeHSrIuPXf7+M5Wt3HPyKsvVQvw9sPRNGD+Vr75t4wMVvvPFGBg0axN69e5k+fTrnn38+V155JQsWLODwww9ny5YtAHzzm9+kf//+PP/88wBs3bq11VBefvll5s+fTzqdZseOHSxYsICSkhLmz5/Pl7/8Ze655x7mzZvH66+/zpIlSygpKWHLli0MHDiQT37yk2zcuJGqqipuuukm5syZc/DfjYgUjW6V4DtMqiSU8Ov3Qc3OUJVzgLtN/uu//ov77rsPgDVr1jBv3jxmzJjRcG/5oEGDAJg/fz6//vWvGz43cODAVkO5+OKLSafTAGzfvp2PfexjvPLKK5gZdXV1Deu96qqrGqpwctu77LLLuO2225gzZw5PPfUUt956a1u/CREpYt0qwX/tvGM7doU734Kda8NdOP0ObXGRxx9/nPnz5/PUU0/Rq1cvTj/9dCZPntxQfZLP3Vu8LTH/veb3pPfu3bth+qtf/SozZ87kvvvuY9WqVZx++ukHXO+cOXM477zzqKio4OKLL1Ydvog00XWuMiahz5BwZ82u9bBnS4uLbN++nYEDB9KrVy9efPFF/vrXv1JTU8Of//xnXn/9dYCGKpozzzyTH/3oRw2fzVXRDB06lBUrVpDNZhvOBPa3rREjRgBw8803N7x/5pln8tOf/pT6+vom2xs+fDjDhw/nW9/6VkO9vohITs9O8GYwYBSU9YFtb4QLsM2cffbZ1NfXM2nSJL761a9y8sknU1VVxbx587jooouYPHkyH/zgBwH4yle+wtatW5k4cSKTJ0/mscceA+CGG27g3HPPZdasWRx6aMtnCgBf/OIXue666zj11FPJZDIN73/84x9n9OjRTJo0icmTJ3P77bc3zLvkkksYNWoUEyZM6KhvRUSKRJfqk3XatGnevMOPFStWcMwxx8S74Uw9bHop3F55yHgoKY93ex3oU5/6FFOnTuWKK67Y7zKd8h2KSCLMbJG7T2tpXs8uweekS2DQuHCxdctKyGZa/0wXcMIJJ/Dcc89x6aVvawFCRKR7XWSNVWkFDBwDW16D3Ruh77CkI2rVokWLkg5BRLowleDzVfSD8n6wa0O4V15EpBtTgm+u33DwTGjfRkSkG1OCb660EioHwq6NkKlNOhoRkXZTgm9J3+hWxp0qxYtI96UE35KScug1GPZsgvp9TRoLExHpLpTg96fvsNCc8I71SUciItIuSvD7ky4NzQvva2wR0t255pprmDhxIscddxx33nknAOvWrWPGjBlMmTKFiRMn8sQTT5DJZLj88ssblv3+97+f1J6ISA/Vve6D/9O1sP75jl3nsOPgPTe0PK/PENi9CQhP+957770sXbqUZ599lk2bNjF9+nRmzJjB7bffzllnncU//dM/kclk2LNnD0uXLuXNN9/khRdeAGDbtm0dG7eISCvi7HT7KDNbmjfsMLOr49peLFIl0Hdo1DPUTp588kk+/OEPk06nGTp0KKeddhrPPPMM06dP56abbuL666/n+eefp2/fvowdO5aVK1fy6U9/mgceeIB+/folvTci0sPE2WXfS8AUADNLA28C+29KsRD7K2nHqVdVGO9Yh2db7vd1xowZLFiwgD/84Q9cdtllXHPNNXz0ox/l2Wef5cEHH+THP/4xd911FzfeeGMnBi4iPV1n1cGfAbzm7qs7aXsdJ5UKrU7W7WbGydO48847yWQybNy4kQULFnDiiSeyevVqhgwZwpVXXskVV1zB4sWL2bRpE9lslve///1885vfZPHixUnviYj0MJ1VB/8h4I6WZpjZXGAuwOjRozspnLYySJdx4axpPLXoWSZPnoyZ8e///u8MGzaMW265he985zuUlpbSp08fbr31Vt58803mzJlDNir1f/vb3054H0Skp4m9uWAzKwPWAse6+wGfHEqsueBC7Nkc2owfOBYq+ycdTZt0me9QRDpc0s0FvwdY3Fpy7/IqB0G6DHauCxddRUS6uM5I8B9mP9Uz3YpZePipfi/s25F0NCIirYo1wZtZL+DdwL0Hs54u0+tUNyzFd5nvTkQ6XawJ3t33uPtgd9/e3nVUVFSwefPmrpGompTi271Lncbd2bx5MxUVFUmHIiIJ6PJPso4cOZLq6mo2btyYdCiBO+zcCtXbukWvTxUVFYwcOTLpMEQkAV0+wZeWlnL44YcnHUZTS5+F31wFH7odjj4n6WhERFqkxsba47iLYdBYePzb3aYuXkR6HiX49kiXwIwvhobPXvpj0tGIiLRICb69VIoXkS5OCb698kvxL/4h6WhERN5GCf5gHHcxDBwDf/lh0pGIiLyNEvzBSJfA9CthzV87viMSEZGDpAR/sKZeAiWV8MzPk45ERKQJJfiDVTkQjns/PHcX7N2WdDQiIg2U4DvC9Cuhbg882/3bVBOR4qEE3xGGT4GR00M1jW6ZFJEuQgm+o0y/Eja/CisfTzoSERFACb7jTDgfeg3WxVYR6TKU4DtKaQUc/9HQdMG2NUlHIyKiBN+hpv19GC+6OdEwREQg/h6dBpjZ3Wb2opmtMLN3xLm9xA0YDePPhsW3QH1N0tGISA8Xdwn+P4EH3P1oYDKwIubtJW/6FbB7Iyz/XdKRiEgPF1uCN7N+wAzgFwDuXuvu2+LaXpcxdlZoZfKZnyUdiYj0cHGW4McCG4GbzGyJmf3czHo3X8jM5prZQjNb2GW65TsYqRRM/zis+Rusey7paESkB4szwZcAxwM/cfepwG7g2uYLufs8d5/m7tOqqqpiDKcTTfmI2qcRkcTFmeCrgWp3/1v0+m5Cwi9+lQNh4kWw7D5dbBWRxMSW4N19PbDGzI6K3joDWB7X9rqcYy+Emh16slVEEhP3XTSfBn5lZs8BU4B/jXl7Xcfhp0F5f1j2m6QjEZEeqiTOlbv7UmBanNvoskrK4Oj3wkt/gPra8FpEpBPpSdY4TbgA9m2H1xckHYmI9EBK8HEaNxPK+sLy3yQdiYj0QErwcSoph6PeAy/eD5m6pKMRkR6mTQnezFLRE6pSqGMvgL1bYdUTSUciIj1MqwnezG43s37RU6jLgZfM7Jr4QysS42ZBWR9Y/tukIxGRHqaQEvwEd98BXAD8ERgNXBZnUEWltBLGnwUrfg+Z+qSjEZEepJAEX2pmpYQE/1t3rwPU8WhbTDgf9myG1f+XdCQi0oMUkuD/B1gF9AYWmNlhwI44gyo6R7wbSnupmkZEOlWrCd7d/8vdR7j7ez1YDczshNiKR1kvOPLMUE2TzSQdjYj0EIVcZP1MdJHVzOwXZrYYmNUJsRWXCefD7g3wxlNJRyIiPUQhVTR/H11kPROoAuYAN8QaVTE68szQhLCqaUSkkxSS4C0avxe4yd2fzXtPClXeB46cHbryy2aTjkZEeoBCEvwiM3uIkOAfNLO+gDJUe0y4AHatD709iYjErJAEfwWhJ6bp7r4HKCNU00hbjT8L0uWqphGRTlHIXTRZYCTwFTP7D+AUd1dno+1R3heOmB0SvOtRAhGJVyF30dwAfIbQTMFy4B/N7NtxB1a0jj4Hdq6Ft5YlHYmIFLlCOvx4LzAlKsljZrcAS4DrWvugma0CdgIZoN7de2bnH/nGRY8QvPYoDJuYbCwiUtQKbU1yQN50/zZuY6a7T1Fyj/QbDlXHhAQvIhKjQkrw3waWmNljhNsjZ1BA6V0OYNxMeOYXULc3NEYmIhKDQi6y3gGcDNwbDe9w918XuH4HHjKzRWY2t6UFzGyumS00s4UbN24sNO7ubdwsyNToqVYRidV+S/Bmdnyzt6qj8XAzG+7uiwtY/6nuvtbMhgAPm9mL7t6kg1J3nwfMA5g2bVrPuLXksFMgXRaqacap1QcRiceBqmi+e4B5TgHt0bj72mi8wczuA04E1AN1WW8YdRK89ljSkYhIEdtvgnf3g2oxMuoBKuXuO6PpM4FvHMw6i8q4WfDI12HnW9B3aNLRiEgRirPT7aHAk2b2LPA08Ad3fyDG7XUvuaqZlY8nGoaIFK9C7qJpF3dfCUyOa/3d3rBJ0GtwqIef/MGkoxGRIhRnCV4OJJWCsafDysfUbIGIxKItd9E0UeBdNHIg42bBC/fAhuUw9NikoxGRIlPIXTQVwDQg1w78JOBvwDvjDa0HGJvXbIESvIh0sP1W0bj7zOhOmtXA8e4+zd1PAKYCr3ZWgEWt/wg45Cg1WyAisSikDv5od38+98LdXwCmxBZRTzNuFqz+C9TtSzoSESkyhST4FWb2czM73cxOM7OfASviDqzHGDcL6vep2QIR6XCFJPg5wDJCm/BXE9qEV49OHWXMqZAqVTWNiHS4Vu+Dd/d9wPejQTpaWW8YfXK4XVJEpAMV0qPTqWb2sJm9bGYrc0NnBNdjjJsJ65+HXRuSjkREikghVTS/AL5HuC1yet4gHUXNFohIDApJ8Nvd/U/uvsHdN+eG2CPrSYZNhspBqocXkQ5VSFs0j5nZdwidfdTk3tSTrB0o12zBa1GzBWZJRyQiRaCQBH9SNM7vU7Wg9uClDcbNgmX3qtkCEekwhdxFc1DtwkuBjjgjjF9+UAleRDpEQc0Fm9k5wLGEdmkAcHd13tGR+g2HQyeHBP+uzyUdjYgUgUJuk/wp8EHg04TGxi4GDos5rp5p/Hug+mnYrWvYInLwCrmL5hR3/yiw1d2/DrwDGFXoBswsbWZLzOz+9gbZY4w/CzwLrz6cdCQiUgQKSfB7o/EeMxsO1AGHt2Ebn0Ft1xTm0CnQZxi89KekIxGRIlBIgr/fzAYA3wEWA6uAOwpZuZmNBM4Bft7O+HqWVArGnwmvPgL1tUlHIyLdXKsJ3t2/6e7b3P0eQt370e7+zwWu/wfAF4Hs/hYws7lmttDMFm7cuLHA1Rax8e+B2p3wxl+SjkREurk29cnq7jXuvr2QZc3sXGCDuy9qZZ3zos5EplVVVbUlnOI09jRIl8NLDyQdiYh0c3F2un0q8D4zWwX8GphlZrfFuL3iUNY7JPmX/6TOuEXkoMSW4N39Oncf6e5jgA8Bj7r7pXFtr6iMPwu2roJNLycdiYh0Y4XcB3+PmZ1jZnGW9iXf+LPD+GVV04hI+xWStH8CfAR4xcxuMLOj27oRd3/c3c9tc3Q9Vf+RMOw41cOLyEEp5C6a+e5+CXA84RbJh83sL2Y2x8xK4w6wxxp/Nqz5K+zZknQkItJNFVTtYmaDgcuBjwNLgP8kJHw9chmX8e+Jnmqdn3QkItJNFVIHfy/wBNALOM/d3+fud7r7p4E+cQfYYw2fCr2H6KlWEWm3QlqT/JG7t9jVkLtPa+l96QC5p1qX/x4ydZBWbZiItE0hVTTHRE0VAGBmA83s/8UXkjQYfzbUbIc3nko6EhHphgpJ8Fe6+7bcC3ffClwZW0TSaOxMSJeFNuJFRNqokASfMmvsJNTM0kBZfCFJg/I+MOZdqocXkXYpJME/CNxlZmeY2SxCS5K6QbuzHPUe2PIabHol6UhEpJspJMF/CXgU+ATwSeARQguR0hnGnxXGK36fbBwi0u0U0ul2lvA060/iD0feZsBoGHUyLL0d3vlZaKwtExE5oELugz/SzO42s+VmtjI3dEZwEpl6KWx+BaqfSToSEelGCqmiuYlQeq8HZgK3Ar+MMyhp5tgLoLQXLNHXLiKFKyTBV7r7I4C5+2p3vx6YFW9Y0kR5Xzj2QnjhPqjdnXQ0ItJNFJLg90VNBb9iZp8yswuBITHHJc1NuSR05bf8d0lHIiLdRCEJ/mpCOzT/CJwAXAp8LMaYpCWHnQKDxsLSXyUdiYh0EwdM8NFDTX/n7rvcvdrd57j7+939r50Un+SYwZSPwKonYMvrSUcjIt3AARO8u2eAE/KfZC2UmVWY2dNm9qyZLTOzr7c7SgkmfwSwcMukiEgrCqmiWQL81swuM7OLckMBn6sBZrn7ZGAKcLaZnXwQsUr/EXDEGSHBZzNJRyMiXVwhCX4QsJlw58x50dBq93se7IpelkaDtzNOyZlyCeyohtf/nHQkItLFFfIk65z2rjyqw18EHAH82N3/1t51SeToc6ByICy5DcbpblUR2b9WE7yZ3UQLJW93//vWPhvV4U+J2pO/z8wmuvsLzdY/F5gLMHr06ALD7sFKyuG4i2HRLbB3a0j2IiItKKSK5n7gD9HwCNAP2HXATzQTtSf/OHB2C/Pmufs0d59WVVXVltX2XFMvhUwNPH930pGISBfWaoJ393vyhl8BfwdMbO1zZlaV6wnKzCqB2cCLBxmvABw6GYYep3viReSACinBN3ckUEhdyqHAY2b2HPAM8LC739+O7UlLpl4Ka5fAW8uSjkREuqhCWpPcaWY7cgPwe0Ib8Qfk7s+5+1R3n+TuE939Gx0RsESOuxhSpaEuXkSkBYXcRdO3MwKRNuo9GCZ9EBbdDKd8GgaMSjoiEeliCinBX2hm/fNeDzCzC2KNSgoz87owfvzbycYhIl1SIXXwX3P37bkX0R0xX4stIilc/5Fw0j+EJ1tVFy8izRSS4FtaptWqHekk7/wsVPSD+WrqR0SaKiTBLzSz75nZODMba2bfJzydKl1Br0Hwrs/DKw/CqieTjkZEupBCEvyngVrgTuAuYC/wyTiDkjY6cS70GwEP/zO4mvsRkaCQu2h2A9d2QizSXqWVMPPL8NtPworfwYTzk45IRLqAQu6ieTj3RGr0eqCZPRhrVNJ2kz8MVceEuvhMXdLRiEgXUEgVzSHRnTMAuPtW1Cdr15NKw+zrYctrsPjWpKMRkS6gkASfNbOGpgnM7DDUrnvXNP4sGH0KPH4D1LSpPTgRKUKFJPh/Ap40s1+a2S+BBcB18YYl7WIG7/467N4Af/3vpKMRkYQV0prkA8DxNN5Fc4K7qw6+qxp1IhzzPnjiu7DuuaSjEZEEFdqaZAbYAGwHJpjZjPhCkoN2zvegchDcdVnoFEREeqRC7qL5OKFa5kHg69H4+njDkoPSpwr+7hbY/ibcdxVks0lHJCIJKKQE/xlgOrDa3WcCU4GNsUYlB2/UiXDWv8LLD8CT3006GhFJQCEJfp+77wMws3J3fxE4Kt6wpEOceGVoN/7Rf4HXHk06GhHpZIUk+OroQaffAA+b2W+BtXEGJR3EDM77T6g6Gu6+AratSToiEelEhdxFc6G7b3P364GvAr8ALmjtc2Y2ysweM7MVZrbMzD5z0NFK25X1hg/eFp5u/d+PQX1N0hGJSCdpU5+s7v5nd/+du9cWsHg98Hl3PwY4GfikmU1oT5BykA45Ai74b3hzETxwrRokE+kh2tPpdkHcfZ27L46mdwIrgBFxbU9aMeF9cOpnYOGNcP9n1V6NSA/QKR13mNkYwt03f2th3lxgLsDo0aObz5aOdMb1YGl48nuwbTVcfDNU9G/tUyLSTcVWgs8xsz7APcDV7r6j+Xx3n+fu09x9WlVVVdzh9GypFMz+GrzvR/D6AvjFWbB1ddJRiUhMYk3wZlZKSO6/cvd749yWtMHxl8Gl98LOtfDzM6B6YdIRiUgMYkvwZmaEO25WuPv34tqOtNPY0+CK+VDaC24+B5b9JumIRKSDxVmCPxW4DJhlZkuj4b0xbk/aqmo8XPkoHDo53EL5xy+qmWGRIhLbRVZ3fxKwuNYvHaT3IfDR34X+XJ+eBy//KTwcNW5W0pGJyEGK/SKrdAOlFfDef4c5f4J0OfzywtC/695tSUcmIgdBCV4aHfYOuOpJeOdnYekd8OOTYMX9SUclIu2kBC9NlVaEvl2vfBR6V8Gdl8DPzoCnfwZ7tiQdnYi0gRK8tGz4FJj7GJx9A9TtgT9+Af5jPNzxEVj+W7VpI9INdMqTrNJNpUvh5E/ASVfB+ufhuTvh+f+Fl/4QnoAdfzYcdmoYBo8LrVeKSJdh3oUanpo2bZovXKiHbrq0TD2sfDwk+9cehT2bwvt9hsJhp4RkP+qk0ERxSVmioYr0BGa2yN2ntTRPJXhpm3QJHDk7DO6w6RVY/X+w+i9hvOy+sFyqFIYcDcMmwbDjovFEtX0j0omU4KX9zMLDUlXjYdqckPC3vQHVz4QqnfXPwysPwdJfNX5m8BEw4oQwDD8+JP/SiuT2QaSIKcFLxzGDgYeF4bgPNL6/862Q7NcthbVLYOWfQxUPQKoEhk6E4VNhxPFhXHVMOFMQkYOi/yKJX9+hYThyduN7O9aGDkhywwv3wqKbwrySilClM3xq+FxJBaTLoKQ8PIhVUga9h8DQCVA5MJl9EukGlOAlGf2Gh+GY88LrbBa2vg5vLg6l/LWLYckvwy2aB1zPyJDohx4bzgT6jYDaXbBvO+zbBvt2hOm6vdBnSJjfb3jjuKxX7LsqkhQleOkaUqlwq+XgcTDp4vBeNguZmnDPfaY2jOtrwns71sJbL8Bby+GtZeGOnmx9y+tOl0FJJdRsf/u8igEwcAwMGguDDo/GY2Hg4eGAkErHtccisVOCl64rlYJUJZRWvn3e0GPhyHc3vq6vhc2vwM51UN4/3K2TG3IXcev2hgNDw1AN29+EravCWcPy34Jn8jZiUN4vrKOyfzgYVPQP1UK9BkGvwWGojKYrB4ZOzsv7QGlvXUeQxOkvUIpDSVlUTXPs/pcprWw8S2hJpi7cBbT1ddjyOuzeGBpc27e9scpny8rQZMPeLeGs4oAxVYSEX9Y7HBwqB0JlNK4YEKbTZYCBpaIHxSyMKwZA/xGNVUnp0jZ/JSJK8CI56dIDHwDyuUPtbtizOST7PZvDwaB2V3i/dnfjdM3OcIDYuxU2vBjGe7dCttCOzy08SNZ/REj82fowZOqi6bqwTO7gUTkwnFVUDgxnHJnacPZStycMtXugfi/0OiTctjp4XBj3rtLTyEVGCV6kPcxCVUx5n3BbaFu5h2SbqQM8vHaPprPhALC9Gna8GaqRctVJe7eGA1GqNFwgTpWGW03xcIDZ8GI44Ozd2vI1iZKKcCZTUhnOUPIPMmV9YfDYcHDwbOOQzYRxqiSqohrQeAZSMQAq+oUzkCZnItF3lKkP10wytaEaLVMb9jmVCr2J5eLJxZQubXomkxunSsLyuTOist7hta6RHFBsCd7MbgTOBTa4+8S4tiPSLZmFJLU/fYZA1VHtX79745lDSXmURJslxEw9bF8DW16Dzbnh1fA5S4UhlYaSNFg6HDC2V4eL23u3Qe3O9sfXUdLl0UEhHV2zKYmm040Hp2x9uLaSjYbSiui6yaDGM51eg8L1ltKKcNBpOBCWh2q0hrOl3JlTXVhX7vbdhlt4yxur5sr7hnWW9w3vJ3B2FGcJ/mbgR8CtMW5DRFpiFkrWFf32v0y6JLpz6HA4Yvb+l9ufTH04gNTsCMkUojORLA1nJenSvGcYShsTcjYTqonq8ob6faGE33AmkzfO1kfVS7sbh1yVUy6JZzNRIq8Pd2DlSv6pdF7iT4Vt5a6j7FwHG1aE6doYu6tMlTYm+vwzndxZSu8quPKRDt9snF32LTCzMXGtX0QSli6B3oPD0ObPlkYl6S70oFo2Gw4y+UNddNBJR1VhqZLGaUuHknz+7bv1NeFzuWsvNTvDAbBmZ3gmI1NLOGiRdyDMhpJ+DBKvgzezucBcgNGjRyccjYj0WKlUuK5RRA+/Jd7hh7vPc/dp7j6tqqoq6XBERIpG4gleRETioQQvIlKkYkvwZnYH8BRwlJlVm9kVcW1LRETeLs67aD4c17pFRKR1qqIRESlSSvAiIkVKCV5EpEgl/qCTdE3uzp7aDLtr6sm4Y4R2NMwIU7lmNXIP5Tk4DkAm69RlnNr6LHWZLDXROJN1UmaUpo10yihJpSiJpmvrw3I19ZkwrgvTYfkUZSVhnBsgxLe3NsPeugz76sJ0fdYpL0lRXpKmvDR/nMIwUgZmFp5ib7Ij0c40EZZPNSwfxu5Qm8k27F9tfTSdddzDOrLuje2HETWTYkbKwv7mxplsltqMU5+J1hVNZz0XQeN3novbrDGudF5s6VSzwYx02mipBRQHstHvVJ/NUp9x6jJZ6rNOfdbJNh972LfwW4TfoCydorQk7EtdxsNvV5elNpONxploH5p/740xljQZh7+H8PuF364smk6njKw79RknE8WUaRZb1sPfXu67b/h7jb6vXBy5v9esezTduHyTXz/vi8v9dqn89Vn4/eqz/rbvDwjffwrSqRTp6PfP/X80iT/6m/3QiR3/oKcSfA+ybU8tS97YxsZdNWzZXcuW3bVs3lXLlt01bNlTx659deyqqWd3TYbdtfUt/tGLFCqdCgeXxkSadERd1yF9ypXgpe3Wbd/LQ8ve4sFl6/nb61vIZBv/y8pLUgzuXcagPmUM7FXGyAGV9C5P06e8lD7laXqXl9C7vISSlDUppXtUas8VcCwqHeVKmimzhlJeadooLUlRng6lsIxHJZdMrhQTSj1l6RTlpem3ld7Aqa0PpaNQws1Slwn70KssTWVpmorSNJXRdOPZQDgT2FfXeEbQEHvefmTdsbyiWvPyfEPpMNv42oCyklCCbSjNloR9Jb+0SmMpMBttK5v1hu/APSTB0nTj2UlJ2ihNpUhFRc387xsazwyy7tEQSuLZaJ1ZbywdZvJKky3JlZ5L0ylKcuO8s6vcGUA6KnEDb/sd6uqzDWdNZc1+u9xnGr7PJrHnSrLZJiXa3JlcbX3uTCBDbSacYTQp8adDiT9t1uTsKGWQSln+CWbednO/d3Q2RmMJP3eGlB9rTvgKPe83bPwdcr9futn3B1GPk954FpTJhm03nrWkSKUI33Wz76qjKMEXoTc27+H3z63loWXrebY69EM6rqo3/zBjLKcfNYRD+1cwqHcZvcrSTf6oRVoTDrrt01BF0+Qwqvbc46QEXyRq67M8tHw9v356DU++ugmAySP7c81ZR3HWscM4YkifhCMUkc6mBN/Nrdy4izufWcPdi6rZvLuWEQMq+dy7x/P+E0YyYkALnVWLSI+hBN9NLVq9hR/Mf4UnXtlEScqYfcxQPnTiKN51ZFVs9Xki0r0owXczz1dv57sPv8TjL23kkD5lXHPWUVw8bSRD+lYkHZqIdDFK8N3Ei+t38L2HXuah5W8xoFcp177naD76jsPoVaafUERapuzQhbk7T7++hVufWs0fX1hHn7ISPjt7PH//zjH0rShNOjwR6eKU4LugrbtruWdxNXc8/QavbdxN34oSPnHaOObOGMuAXmVJhyci3YQSfBeRzTrPrNrCHU+/wR9fWE9tfZbjRw/gOx+YxLmThlNZpvuFRaRtlOATVJfJ8vTrW3jghfU8tHw9b+2ooW95CR+aPooPnziaYw6Np6d1EekZYk3wZnY28J+Ex9V+7u43xLm9rs7d2bCzhmfXbOPBZW8xf8VbbN9bR0VpitPGV3H2xGGcdewwXTgVkQ4RWyYxszTwY+DdQDXwjJn9zt2Xx7XNjuZR+x51GacumyWTGzdrSyXXdkZNff50hi27a1m9eQ+rN+/hjS27eWPLHvbVZQHoV1HC7GOGctbEYcw4skpVMCLS4eIsKp4IvOruKwHM7NfA+UCHJ/hzf/gEe2szjQ1iRY0Z+duaf23kDY00hQaB8psbrYsaN6rNZA86torSFIcN6s1hg3sz48gqRg/uxRFD+jB9zKCo2VsRkXjEmeBHAGvyXlcDJzVfyMzmAnMBRo9uX3OZR1T1oS7jzdp+bmwlbn9SUZvZzVujK02nKC1JUZoyStKNrSKWpIx0Oryf36JdaAkx1aQlxPKSFP0rS6nqW64GvUQkEXEm+P31M9D0Dfd5wDyAadOmtavF6B98aGp7PiYiUtTirCOoBkblvR4JrI1xeyIikifOBP8McKSZHW5mZcCHgN/FuD0REckTWxWNu9eb2aeABwm3Sd7o7svi2p6IiDQV6w3X7v5H4I9xbkNERFqm+/RERIqUEryISJFSghcRKVJK8CIiRcrc2/VsUSzMbCOwup0fPwTY1IHhdBfa755F+92zFLLfh7l7VUszulSCPxhmttDdpyUdR2fTfvcs2u+e5WD3W1U0IiJFSgleRKRIFVOCn5d0AAnRfvcs2u+e5aD2u2jq4EVEpKliKsGLiEgeJXgRkSLV7RO8mZ1tZi+Z2atmdm3S8cTJzG40sw1m9kLee4PM7GEzeyUaD0wyxo5mZqPM7DEzW2Fmy8zsM9H7xb7fFWb2tJk9G+3316P3i3q/c8wsbWZLzOz+6HVP2e9VZva8mS01s4XRe+3e926d4PM69n4PMAH4sJlNSDaqWN0MnN3svWuBR9z9SOCR6HUxqQc+7+7HACcDn4x+42Lf7xpglrtPBqYAZ5vZyRT/fud8BliR97qn7DfATHefknf/e7v3vVsnePI69nb3WiDXsXdRcvcFwJZmb58P3BJN3wJc0Jkxxc3d17n74mh6J+GffgTFv9/u7ruil6XR4BT5fgOY2UjgHODneW8X/X4fQLv3vbsn+JY69h6RUCxJGeru6yAkQ2BIwvHExszGAFOBv9ED9juqplgKbAAedvcesd/AD4AvAtm893rCfkM4iD9kZovMbG70Xrv3PdYOPzpBQR17S/dnZn2Ae4Cr3X2HWUs/fXFx9wwwxcwGAPeZ2cSEQ4qdmZ0LbHD3RWZ2esLhJOFUd19rZkOAh83sxYNZWXcvwatjb3jLzA4FiMYbEo6nw5lZKSG5/8rd743eLvr9znH3bcDjhOsvxb7fpwLvM7NVhCrXWWZ2G8W/3wC4+9povAG4j1AN3e597+4JXh17h/39WDT9MeC3CcbS4SwU1X8BrHD37+XNKvb9ropK7phZJTAbeJEi3293v87dR7r7GML/86PufilFvt8AZtbbzPrmpoEzgRc4iH3v9k+ymtl7CXV2uY69/yXZiOJjZncApxOaEH0L+BrwG+AuYDTwBnCxuze/ENttmdk7gSeA52msk/0yoR6+mPd7EuGCWppQELvL3b9hZoMp4v3OF1XRfMHdz+0J+21mYwmldgjV57e7+78czL53+wQvIiIt6+5VNCIish9K8CIiRUoJXkSkSCnBi4gUKSV4EZEipQQv0gHM7PRcy4ciXYUSvIhIkVKClx7FzC6N2llfamb/EzXotcvMvmtmi83sETOripadYmZ/NbPnzOy+XDvcZnaEmc2P2mpfbGbjotX3MbO7zexFM/uV9YQGc6RLU4KXHsPMjgE+SGjQaQqQAS4BegOL3f144M+EJ4QBbgW+5O6TCE/S5t7/FfDjqK32U4B10ftTgasJfROMJbSrIpKY7t6apEhbnAGcADwTFa4rCQ03ZYE7o2VuA+41s/7AAHf/c/T+LcD/Rm2FjHD3+wDcfR9AtL6n3b06er0UGAM8GfteieyHErz0JAbc4u7XNXnT7KvNljtQ+x0HqnapyZvOoP8vSZiqaKQneQT4QNTWdq6vy8MI/wcfiJb5CPCku28HtprZu6L3LwP+7O47gGozuyBaR7mZ9erMnRAplEoY0mO4+3Iz+wqhx5wUUAd8EtgNHGtmi4DthHp6CE2z/jRK4CuBOdH7lwH/Y2bfiNZxcSfuhkjB1Jqk9Hhmtsvd+yQdh0hHUxWNiEiRUgleRKRIqQQvIlKklOBFRIqUEryISJFSghcRKVJK8CIiRer/A3NfaPNF1/2CAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(model.history['accuracy'])\n",
    "plt.plot(model.history['loss'])\n",
    "plt.title('model accuracy and loss')\n",
    "plt.ylabel('accuracy and loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['accuracy', 'loss'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/122 [==============================] - 7s 33ms/step - loss: 0.7617 - accuracy: 0.6737\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7616528272628784, 0.6736950278282166]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"C:/Users/Admin/Downloads/nextword2.h5\")\n",
    "vocab_array = np.array(list(tokenizer.word_index.keys()))\n",
    "\n",
    "# Importing the Libraries\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pickle\n",
    "# Load the model and tokenizer\n",
    "model.load_weights(\"C:/Users/Admin/Downloads/nextword2.h5\")\n",
    "tokenizer = pickle.load(open(\"C:/Users/Admin/Downloads/tokenizer2.pkl\", 'rb'))\n",
    "def make_prediction(text, n_words):\n",
    "    for i in range(n_words):\n",
    "        text_tokenize = tokenizer.texts_to_sequences([text])\n",
    "        text_padded = tf.keras.preprocessing.sequence.pad_sequences(text_tokenize, maxlen=14)\n",
    "        prediction = np.squeeze(np.argmax(model.predict(text_padded), axis=-1))\n",
    "        prediction = str(vocab_array[prediction - 1])\n",
    "        print(vocab_array[np.argsort(model.predict(text_padded)) - 1].ravel()[:-3])\n",
    "        text += \" \" + prediction\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name='embedding_input'), name='embedding_input', description=\"created by layer 'embedding_input'\"), but it was called on an input with incompatible shape (None, 14).\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "['out' 'angry' 'refuse' ... 'sighs' 'unusual' 'kind']\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "['investigate' 'flow' 'nightgown' ... \"she's\" 'left' 'kind']\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "['urge' 'much' 'startlement' ... 'or' 'we' 'once']\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "['much' 'urge' 'startlement' ... 'slide' 'once' 'usually']\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "['much' 'extreme' 'obviously' ... 'leaves' 'stuff' 'thanks']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'from while stuff thanks clever kitchen'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_prediction(\"from\",5)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
